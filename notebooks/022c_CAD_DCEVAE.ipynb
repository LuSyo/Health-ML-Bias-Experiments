{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disentangled Causal Effect Variational Autoencoder"
      ],
      "metadata": {
        "id": "U0xQR9NH0zm0"
      },
      "id": "U0xQR9NH0zm0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "- data/heart_disease_cleaned.csv\n",
        "\n",
        "**Outputs:**\n",
        "- DCEVEA model\n",
        "- data/fair_disease_dcevae.csv\n",
        "- data/cf_disease_dcevea.csv"
      ],
      "metadata": {
        "id": "z16vyaLs1AP5"
      },
      "id": "z16vyaLs1AP5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "zgs3GX9e1YvV"
      },
      "id": "zgs3GX9e1YvV"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = userdata.get('PROJECT_ROOT')\n",
        "except ImportError:\n",
        "  PROJECT_ROOT = '/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313eapZ0098S",
        "outputId": "9d541cab-dcf7-44ae-cc3d-dcad5859a26e"
      },
      "id": "313eapZ0098S",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as utils\n",
        "import re\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "7xdN4Hk31hsu"
      },
      "id": "7xdN4Hk31hsu",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_PATH = PROJECT_ROOT + '/models/dcevae.pt'\n",
        "SEED = 4\n",
        "BATCH_SIZE = 32\n",
        "UC_DIM = 3\n",
        "UD_DIM = 3\n",
        "H_DIM = 5\n",
        "ACT_FN = 'relu'\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "CORR_RECON_ALPHA = 1\n",
        "DESC_RECON_ALPHA = 1\n",
        "PRED_ALPHA = 1\n",
        "\n",
        "args = Namespace(\n",
        "    device=DEVICE,\n",
        "    model_path=MODEL_PATH,\n",
        "    seed=SEED,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    uc_dim=UC_DIM,\n",
        "    ud_dim=UD_DIM,\n",
        "    h_dim=H_DIM,\n",
        "    act_fn=ACT_FN,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    lr = LEARNING_RATE,\n",
        "    corr_a = CORR_RECON_ALPHA,\n",
        "    desc_a = DESC_RECON_ALPHA,\n",
        "    pred_a = PRED_ALPHA\n",
        ")"
      ],
      "metadata": {
        "id": "Ix6--YoyY0pT"
      },
      "id": "Ix6--YoyY0pT",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "uMw8HIuI6GvP"
      },
      "id": "uMw8HIuI6GvP"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(X_ind, X_desc, X_corr, X_sens, Y, index, batch_size=32):\n",
        "  X_ind_fact = X_ind[index]\n",
        "  X_desc_fact = X_desc[index]\n",
        "  X_corr_fact = X_corr[index]\n",
        "  X_sens_fact = X_sens[index]\n",
        "  Y_fact = Y[index]\n",
        "\n",
        "  # Permuted set for the discriminator\n",
        "  permuted_indices = np.random.permutation(X_ind_fact.shape[0])\n",
        "  X_ind_fake = X_ind[permuted_indices]\n",
        "  X_desc_fake = X_desc[permuted_indices]\n",
        "  X_corr_fake = X_corr[permuted_indices]\n",
        "  X_sens_fake = X_sens[permuted_indices]\n",
        "  Y_fake = Y[permuted_indices]\n",
        "\n",
        "  X_ind_tensor = torch.tensor(X_ind_fact, dtype=torch.float32)\n",
        "  X_desc_tensor = torch.tensor(X_desc_fact, dtype=torch.float32)\n",
        "  X_corr_tensor = torch.tensor(X_corr_fact, dtype=torch.float32)\n",
        "  X_sens_tensor = torch.tensor(X_sens_fact, dtype=torch.float32)\n",
        "  Y_tensor = torch.tensor(Y_fact, dtype=torch.float32)\n",
        "  X_ind_tensor_2 = torch.tensor(X_ind_fake, dtype=torch.float32)\n",
        "  X_desc_tensor_2 = torch.tensor(X_desc_fake, dtype=torch.float32)\n",
        "  X_corr_tensor_2 = torch.tensor(X_corr_fake, dtype=torch.float32)\n",
        "  X_sens_tensor_2 = torch.tensor(X_sens_fake, dtype=torch.float32)\n",
        "  Y_tensor_2 = torch.tensor(Y_fake, dtype=torch.float32)\n",
        "\n",
        "  dataset = utils.TensorDataset(X_ind_tensor, X_desc_tensor, X_corr_tensor, X_sens_tensor, Y_tensor,\n",
        "                                X_ind_tensor_2, X_desc_tensor_2, X_corr_tensor_2, X_sens_tensor_2, Y_tensor_2)\n",
        "  loader = utils.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def make_bucketed_loader(dataset, map, val_size=0.1, test_size=0.1, batch_size=32, seed=4):\n",
        "  '''\n",
        "    Creates train, validation and test DataLoader for the given dataset, \\\n",
        "    separating features into independent, sensitive, descendant, and correlated features.\n",
        "\n",
        "    Input:\n",
        "      - dataset: a pandas DataFrame\n",
        "      - map: a dictionary mapping feature names to buckets\n",
        "      - val_size: the proportion of the dataset to use for validation\n",
        "      - test_size: the proportion of the dataset to use for testing\n",
        "      - batch_size: the batch size for the DataLoader\n",
        "      - seed: a seed for the random number generator\n",
        "\n",
        "    Output:\n",
        "      - train_loader: Training DataLoader\n",
        "      - val_loader: Validation DataLoader\n",
        "      - test_loader: Testing DataLoader\n",
        "      - ind_types: a list of the data types of the independent features\n",
        "      - desc_types: a list of the data types of the descendant features\n",
        "      - corr_types: a list of the data types of the correlated features\n",
        "      - sens_type: the data type of the sensitive feature\n",
        "  '''\n",
        "  np.random.seed(seed=seed)\n",
        "\n",
        "  ## BUCKET DATASET\n",
        "  # Independent, Descendant, Correlated features and Sensitive attributes\n",
        "  col_ind = []\n",
        "  for feature in map['ind']:\n",
        "    col_ind.append(feature['name'])\n",
        "  X_ind = dataset[col_ind].to_numpy()\n",
        "\n",
        "\n",
        "  col_desc = []\n",
        "  for feature in map['desc']:\n",
        "    col_desc.append(feature['name'])\n",
        "  X_desc = dataset[col_desc].to_numpy()\n",
        "\n",
        "  col_corr = []\n",
        "  for feature in map['corr']:\n",
        "    col_corr.append(feature['name'])\n",
        "  X_corr = dataset[col_corr].to_numpy()\n",
        "\n",
        "  col_sens = []\n",
        "  for feature in map['sens']:\n",
        "    col_sens.append(feature['name'])\n",
        "  X_sens = dataset[col_sens].to_numpy()\n",
        "\n",
        "  # Target\n",
        "  Y = dataset[map['target']['name']].to_numpy().reshape(-1, 1)\n",
        "\n",
        "  ## TRAIN-VAL-TRAIN SPLIT\n",
        "  N = X_ind.shape[0]\n",
        "  shuffled_indices = np.random.permutation(N)\n",
        "  val_count = int(N * val_size)\n",
        "  test_count = int(N * test_size)\n",
        "  val_index = shuffled_indices[:val_count]\n",
        "  test_index = shuffled_indices[val_count:val_count+test_count]\n",
        "  train_index = shuffled_indices[val_count+test_count:]\n",
        "\n",
        "  # Training loader\n",
        "  train_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, train_index, batch_size)\n",
        "\n",
        "  # Validation loader\n",
        "  val_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, val_index, batch_size)\n",
        "\n",
        "  # Test loader\n",
        "  test_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, test_index, batch_size)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "tSQZZV156LC9"
      },
      "id": "tSQZZV156LC9",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCEVEA Architecture"
      ],
      "metadata": {
        "id": "ixf1y-38cyV_"
      },
      "id": "ixf1y-38cyV_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network architecture\n",
        "\n",
        "### Network objectives\n",
        "\n",
        "The DCEVAE network uses a min-max core objective structure to minimize the negative Evidence Lower Bound (ELBO) and Total Correlation loss ($\\mathcal{L}_{TC}$), while maximising the objective of the discriminator ($M_D$):\n",
        "\n",
        "$$min_{\\theta, \\phi}\\mathcal{L}_{DCEVAE} := - \\mathcal{M}_{ELBO} + \\beta_{TC}\\mathcal{L}_{TC}$$\n",
        "\n",
        "$$max_{\\psi}\\mathcal{M}_D$$\n",
        "\n",
        "- θ and ϕ are the paramaters for the decoder $q_{\\theta}$ and encoder $p_{\\phi}$ networks respectively\n",
        "- ψ represents the parameters of the discriminator network $D_{\\psi}$\n",
        "- $\\beta_{TC}$ is the Total Correlation scaling parameter\n",
        "\n",
        "#### ELBO\n",
        "\n",
        "Measures how well the decoder reconstructs the inputs (Expected Log-Likelihood, to be maximised) and how close the encoder's distribution of the latent variable is to a standard prior distribution (KL Divergence, to be minimised)\n",
        "\n",
        "$$ELBO = \\mathbb{E}_{q_{\\phi}(u_d|s,x_i,x_d,y)q_{\\phi}(u_c|s,x_i,x_c,y)}[\\log p_{\\theta}(y|s,x_i,u_d,u_c)]+\\mathbb{E}_{q_{\\phi}(u_d|s,x_i,x_d,y)}[\\log p_{\\theta}(y|s,x_i,u_d)]+\\mathbb{E}_{q_{\\phi}(u_c|s,x_i,x_c,y)}[\\log p_{\\theta}(y|s,x_i,u_c)]-KL(q_{\\phi}(u_d|s,x_i,x_d,y)∥p(u_d))-KL(q_{\\phi}(u_c|s,x_i,x_c,y)∥ p(u_c))$$\n",
        "\n",
        "#### Disentanglement Loss\n",
        "To ensure that the latent variables are properly disentangled from the sensitive attribute, the DCEVAE model minimises the Total Correlation loss approximated by the discriminator:\n",
        "$$\\mathcal{L}_{TC} \\approx \\mathbb{E}_{q(s,u_d,u_c)}[\\log \\frac{D_{\\psi}(s,u_d,u_c)}{1-D_{\\psi}(s,u_d,u_c)}]$$\n",
        "\n",
        "while the discriminator maximises its adversary objective:\n",
        "\n",
        "$$M_D = \\mathbb{E}_{q(s,u_d,u_c)}[\\log(D_{\\psi}(s,u_d,u_c))]+\\mathbb{E}_{q(s,u_c)q(u_d)}[\\log(1-D_{\\psi}(s,u_d,u_c))]$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CQ_gxqb2amc"
      },
      "id": "0CQ_gxqb2amc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCEVAE Model Class"
      ],
      "metadata": {
        "id": "h9yLeeDC6DlE"
      },
      "id": "h9yLeeDC6DlE"
    },
    {
      "cell_type": "code",
      "source": [
        "class DCEVAE(nn.Module):\n",
        "  def __init__(self, ind_meta, desc_meta, corr_meta, sens_meta, args):\n",
        "    super(DCEVAE, self).__init__()\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    self.ind_meta = ind_meta\n",
        "    self.desc_meta = desc_meta\n",
        "    self.corr_meta = corr_meta\n",
        "    self.sens_meta = sens_meta\n",
        "\n",
        "    self.embedding_dim = 3\n",
        "    self.embeddings = nn.ModuleDict()\n",
        "\n",
        "    def get_bucket_dim(bucket_meta):\n",
        "      '''\n",
        "        Returns the total dimension of the feature bucket\n",
        "      '''\n",
        "      total_dim = 0\n",
        "      for feature in bucket_meta:\n",
        "        if feature['type'] == 'categorical':\n",
        "          self.embeddings[feature['name']] = nn.Embedding(feature['n'], self.embedding_dim)\n",
        "          total_dim += self.embedding_dim\n",
        "        else:\n",
        "          total_dim += 1\n",
        "      return total_dim\n",
        "\n",
        "    self.ind_dim = get_bucket_dim(ind_meta)\n",
        "    self.desc_dim = get_bucket_dim(desc_meta)\n",
        "    self.corr_dim = get_bucket_dim(corr_meta)\n",
        "    self.sens_dim = get_bucket_dim(sens_meta)\n",
        "    self.target_dim = 1\n",
        "\n",
        "    self.args = args\n",
        "    self.device = args.device\n",
        "    self.uc_dim = args.uc_dim\n",
        "    self.ud_dim = args.ud_dim\n",
        "    self.u_dim = self.uc_dim + self.ud_dim\n",
        "    self.h_dim = args.h_dim\n",
        "    self.batch_size = args.batch_size\n",
        "\n",
        "    # Activation function\n",
        "    if (args.act_fn == 'relu'):\n",
        "      self.act_fn = nn.LeakyReLU()\n",
        "    elif (args.act_fn == 'tanh'):\n",
        "      self.act_fn = nn.Tanh()\n",
        "\n",
        "    # Encoder (X_ind, X_corr, X_sens, Y) -> U_c\n",
        "    input_dim = self.ind_dim + self.corr_dim + self.sens_dim + self.target_dim\n",
        "    self.encoder_corr = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "    self.logvar_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "\n",
        "    # Encoder (X_ind, X_desc, X_sens, Y) -> U_d\n",
        "    input_dim = self.ind_dim + self.desc_dim + self.sens_dim + self.target_dim\n",
        "    self.encoder_desc = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "    self.logvar_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "\n",
        "    # Decoder (U_c, X_ind) -> X_corr\n",
        "    self.decoder_corr = nn.ModuleDict()\n",
        "    for feature in corr_meta:\n",
        "      out_dim = 1 if feature['type'] != 'categorical' else feature.get('n')\n",
        "      self.decoder_corr[feature['name']] = nn.Sequential(\n",
        "          nn.Linear(self.uc_dim + self.ind_dim, self.h_dim),\n",
        "          self.act_fn,\n",
        "          nn.Linear(self.h_dim, out_dim)\n",
        "      )\n",
        "\n",
        "    # Decoder (U_desc, X_ind, X_sens) -> X_desc\n",
        "    self.decoder_desc = nn.ModuleDict()\n",
        "    for feature in desc_meta:\n",
        "      out_dim = 1 if feature['type'] != 'categorical' else feature.get('n')\n",
        "      self.decoder_desc[feature['name']] = nn.Sequential(\n",
        "          nn.Linear(self.ud_dim + self.ind_dim + self.sens_dim , self.h_dim),\n",
        "          self.act_fn,\n",
        "          nn.Linear(self.h_dim, out_dim)\n",
        "      )\n",
        "\n",
        "    # Decoder (U_desc, U_corr, X_ind, X_sens) -> Y\n",
        "    self.decoder_target = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + self.ind_dim + self.sens_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.target_dim)\n",
        "    )\n",
        "\n",
        "    # Discriminator\n",
        "    self.discriminator = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + self.sens_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, 2)\n",
        "    )\n",
        "\n",
        "    self.init_params()\n",
        "\n",
        "  def init_params(self):\n",
        "    '''\n",
        "      Initialise the network parameters.\n",
        "    '''\n",
        "    main_act_fn = \"tanh\" if isinstance(self.act_fn, nn.Tanh) else \"relu\"\n",
        "\n",
        "    for name, module in self.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "        if \"discriminator\" in name:\n",
        "          # Kaiming normal for the LeakyReLU in the Discriminator\n",
        "          nn.init.kaiming_normal_(module.weight, a=0.2, nonlinearity=\"leaky_relu\")\n",
        "        else:\n",
        "          # Xavier normal for Tanh, Kaiming normal for LeakyReLU in the encoders/decoders\n",
        "          if main_act_fn == \"tanh\":\n",
        "            nn.init.xavier_normal(module.weight)\n",
        "          else:\n",
        "            nn.init.kaiming_normal_(module.weight, a=0.01, nonlinearity=\"leaky_relu\")\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.constant_(module.bias, 0)\n",
        "\n",
        "  def discriminate(self, v):\n",
        "    '''\n",
        "      Discriminator forward pass\n",
        "    '''\n",
        "    return self.discriminator(v).squeeze()\n",
        "\n",
        "  def _process_features(self, x, x_meta):\n",
        "    '''\n",
        "      Processes the input features by applying embeddings to the categorical columns\n",
        "    '''\n",
        "    processed = []\n",
        "    for i, feature in enumerate(x_meta):\n",
        "      col = x[:, i]\n",
        "      if feature['type'] == 'categorical':\n",
        "        processed.append(self.embeddings[feature['name']](col.long()))\n",
        "      else:\n",
        "        processed.append(col.unsqueeze(1))\n",
        "    return torch.cat(processed, dim=1)\n",
        "\n",
        "  def encode(self, x_ind, x_desc, x_corr, x_sens, y):\n",
        "    '''\n",
        "      Encoders forward pass\n",
        "    '''\n",
        "    # Process raw tensors\n",
        "    x_ind_p = self._process_features(x_ind, self.ind_meta)\n",
        "    x_desc_p = self._process_features(x_desc, self.desc_meta)\n",
        "    x_corr_p = self._process_features(x_corr, self.corr_meta)\n",
        "    x_sens_p = self._process_features(x_sens, self.sens_meta)\n",
        "\n",
        "    # Correlated path encoder\n",
        "    input_corr = torch.cat((x_ind_p, x_corr_p, x_sens_p, y), dim=1)\n",
        "    h_corr = self.encoder_corr(input_corr)\n",
        "    mu_corr = self.mu_corr(h_corr)\n",
        "    logvar_corr = self.logvar_corr(h_corr)\n",
        "\n",
        "    # Descendant path encoder\n",
        "    input_desc = torch.cat((x_ind_p, x_desc_p, x_sens_p, y), dim=1)\n",
        "    h_desc = self.encoder_desc(input_desc)\n",
        "    mu_desc = self.mu_desc(h_desc)\n",
        "    logvar_desc = self.logvar_desc(h_desc)\n",
        "\n",
        "    return mu_corr, logvar_corr, mu_desc, logvar_desc\n",
        "\n",
        "  def decode(self, u_desc, u_corr, x_ind, x_sens):\n",
        "    '''\n",
        "      Decoders forward pass\n",
        "    '''\n",
        "    # Process raw tensors\n",
        "    x_ind_p = self._process_features(x_ind, self.ind_meta)\n",
        "    x_sens_p = self._process_features(x_sens, self.sens_meta)\n",
        "\n",
        "    # Correlated path\n",
        "    input_corr = torch.cat((u_corr, x_ind_p), dim=1)\n",
        "    x_corr_pred = {feature['name']: self.decoder_corr[feature['name']](input_corr)\\\n",
        "                   for feature in self.corr_meta}\n",
        "\n",
        "    # Descendant path\n",
        "    input_desc = torch.cat((u_desc, x_ind_p, x_sens_p), dim=1)\n",
        "    x_desc_pred = {feature['name']: self.decoder_desc[feature['name']](input_desc)\\\n",
        "                   for feature in self.desc_meta}\n",
        "\n",
        "    # Target\n",
        "    input_target = torch.cat((u_desc, u_corr, x_ind_p, x_sens_p), dim=1)\n",
        "    y_pred = self.decoder_target(input_target)\n",
        "\n",
        "    # Counterfactual\n",
        "    x_sens_cf_p = self._process_features(1 - x_sens, self.sens_meta)\n",
        "    input_desc_cf = torch.cat((u_desc, x_ind_p, x_sens_cf_p), dim=1)\n",
        "    x_desc_cf = {feature['name']: self.decoder_desc[feature['name']](input_desc_cf)\\\n",
        "                   for feature in self.desc_meta}\n",
        "\n",
        "    input_target_cf = torch.cat((u_desc, u_corr, x_ind_p, x_sens_cf_p), dim=1)\n",
        "    y_cf = self.decoder_target(input_target_cf)\n",
        "\n",
        "    return x_corr_pred, x_desc_pred, y_pred, x_desc_cf, y_cf\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    '''\n",
        "      Reparameterisation trick\n",
        "    '''\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std).to(self.device)\n",
        "    return mu + eps * std\n",
        "\n",
        "  def reconstruction_loss(self, v_pred, v, v_meta):\n",
        "    '''\n",
        "      Calculates the total reconstruction loss for the given feature bucket,\\\n",
        "       matching each feature type to the correct loss function\n",
        "    '''\n",
        "    total_recon_L = 0\n",
        "    for i, feature in enumerate(v_meta):\n",
        "      pred = v_pred[feature['name']]\n",
        "      target = v[:, i]\n",
        "\n",
        "      if feature['type'] == 'categorical':\n",
        "        total_recon_L += nn.CrossEntropyLoss(reduction='sum')(pred, target.long())\n",
        "      elif feature['type'] == 'binary':\n",
        "        total_recon_L += nn.BCEWithLogitsLoss(reduction='sum')(pred, target.unsqueeze(1))\n",
        "      else:\n",
        "        total_recon_L += nn.MSELoss(reduction='sum')(pred, target.unsqueeze(1))\n",
        "\n",
        "    return total_recon_L / self.batch_size\n",
        "\n",
        "  def kl_loss(self, mu, logvar):\n",
        "    '''\n",
        "      Calculates the KL divergence for the given latent variable between the \\\n",
        "      approzimate posterior q(u|x,y,a) and the standard normal prior p(u) = N(0,I)\n",
        "    '''\n",
        "    logvar_clamped = torch.clamp(logvar, -10.0, 10.0)\n",
        "\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar_clamped - mu.pow(2) - logvar_clamped.exp())\n",
        "\n",
        "    return kl_div / self.batch_size\n",
        "\n",
        "  def tc_loss(self, u_desc, u_corr, x_ind, x_sens):\n",
        "    # Run the discriminator\n",
        "    # Calculate the TC loss to minimise for the VAE\n",
        "    # Calculate the objective to maximise for the discriminator\n",
        "    return\n",
        "\n",
        "  def fair_loss(self, y, y_cf):\n",
        "    y_cf_sig = nn.Sigmoid()(y_cf)\n",
        "    y_p_sig = nn.Sigmoid()(y)\n",
        "    fair_L = torch.sum(torch.norm(y_cf_sig - y_p_sig, p=2, dim=1))/self.batch_size\n",
        "    return fair_L\n",
        "\n",
        "  def calculate_loss(self, x_ind, x_desc, x_corr, x_sens, y):\n",
        "\n",
        "    # Encode\n",
        "    mu_corr, logvar_corr, mu_desc, logvar_desc = self.encode(\n",
        "        x_ind, x_desc, x_corr, x_sens, y)\n",
        "\n",
        "    # Reparamaterise\n",
        "    u_corr = self.reparameterize(mu_corr, logvar_corr)\n",
        "    u_desc = self.reparameterize(mu_desc, logvar_desc)\n",
        "\n",
        "    # Decode\n",
        "    x_corr_pred, x_desc_pred, y_pred, x_desc_cf, y_cf = self.decode(\n",
        "        u_desc, u_corr, x_ind, x_sens)\n",
        "\n",
        "    # Reconstruction & prediction loss\n",
        "    desc_recon_L = self.reconstruction_loss(x_desc_pred, x_desc, self.desc_meta)\n",
        "    corr_recon_L = self.reconstruction_loss(x_corr_pred, x_corr, self.corr_meta)\n",
        "    y_recon_L = nn.BCEWithLogitsLoss(reduction='sum')(y_pred, y)\n",
        "\n",
        "    recon_L = self.args.desc_a*desc_recon_L + self.args.corr_a*corr_recon_L + self.args.pred_a*y_recon_L\n",
        "\n",
        "    # KL loss\n",
        "    kl_corr_L = self.kl_loss(mu_corr, logvar_corr)\n",
        "    kl_desc_L = self.kl_loss(mu_desc, logvar_desc)\n",
        "\n",
        "    # TC loss\n",
        "\n",
        "    # Counterfactual fairness loss\n",
        "\n",
        "    # ELBO\n",
        "    # elbo = recon_L + kl loss + vae_tc_loss + fair loss\n",
        "\n",
        "    return recon_L + kl_corr_L + kl_desc_L\n",
        "\n"
      ],
      "metadata": {
        "id": "iNE0jzfR1ygY"
      },
      "id": "iNE0jzfR1ygY",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "LFHIW0ml12n3"
      },
      "id": "LFHIW0ml12n3"
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease = pd.read_csv(PROJECT_ROOT + '/data/heart_disease_cleaned.csv',\n",
        "                            dtype={'cp':int, 'ang':int, 'ecg':int, 'fbs':int, 'slope':int})\n",
        "\n",
        "feature_mapping = {\n",
        "    'ind': [{'name':'age','type':'continuous'}], # Features independent of the protected attribute and unconfounded\n",
        "    'desc': [\n",
        "        {'name':'ang','type':'binary'},\n",
        "        {'name':'cp','type':'categorical', 'n':4},\n",
        "        {'name':'ecg','type':'categorical','n':3}], # Features descendant of the protected attribute\n",
        "    'corr': [\n",
        "        {'name':'bp','type':'continuous'},\n",
        "        {'name':'chol','type':'continuous'},\n",
        "        {'name':'mhr','type':'continuous'},\n",
        "        {'name':'st','type':'continuous'},\n",
        "        {'name':'fbs','type':'binary'},\n",
        "        {'name':'slope','type':'categorical', 'n':3}], # Features correlated with the protected attribute\n",
        "    'sens': [{'name':'sex','type':'binary'}], # Sensitive attribute\n",
        "    'target': {'name':'cvd','type':'binary'} # Target outcome\n",
        "}\n",
        "\n",
        "# Bucketed data loaders for training , validation, and test\n",
        "train_loader, val_loader, test_loader = make_bucketed_loader(heart_disease, feature_mapping)\n"
      ],
      "metadata": {
        "id": "et_WZlZs12Hc"
      },
      "id": "et_WZlZs12Hc",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate the DCEVAE model\n",
        "model = DCEVAE(feature_mapping['ind'], feature_mapping['desc'], feature_mapping['corr'],\n",
        "               feature_mapping['sens'], args=args)\n",
        "\n",
        "# Training\n",
        "model.to(args.device)\n",
        "model = model.train()\n",
        "\n",
        "discrim_params = [param for name, param in model.named_parameters() if 'discriminator' in name]\n",
        "main_params = [param for name, param in model.named_parameters() if 'discriminator' not in name]\n",
        "discrim_optimiser = optim.Adam(discrim_params, lr=args.lr)\n",
        "main_optimiser = optim.Adam(main_params, lr=args.lr)\n",
        "\n",
        "for epoch in range(args.n_epochs):\n",
        "  model.train()\n",
        "  for i, (x_ind, x_desc, x_corr, x_sens, y,\n",
        "          x_ind_2, x_desc_2, x_corr_2, x_sens_2, y_2) in enumerate(train_loader):\n",
        "    elbo = model.calculate_loss(x_ind, x_desc, x_corr, x_sens, y)\n",
        "    print(elbo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hOuetAiztsB",
        "outputId": "d1a9da6f-42ab-4c35-aacb-df58acba2b82"
      },
      "id": "-hOuetAiztsB",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(76.5950, grad_fn=<AddBackward0>)\n",
            "tensor(80.4066, grad_fn=<AddBackward0>)\n",
            "tensor(52.2970, grad_fn=<AddBackward0>)\n",
            "tensor(75.0182, grad_fn=<AddBackward0>)\n",
            "tensor(70.4816, grad_fn=<AddBackward0>)\n",
            "tensor(62.2154, grad_fn=<AddBackward0>)\n",
            "tensor(62.9786, grad_fn=<AddBackward0>)\n",
            "tensor(55.6094, grad_fn=<AddBackward0>)\n",
            "tensor(70.8743, grad_fn=<AddBackward0>)\n",
            "tensor(74.2646, grad_fn=<AddBackward0>)\n",
            "tensor(56.8943, grad_fn=<AddBackward0>)\n",
            "tensor(64.8516, grad_fn=<AddBackward0>)\n",
            "tensor(80.5376, grad_fn=<AddBackward0>)\n",
            "tensor(85.7045, grad_fn=<AddBackward0>)\n",
            "tensor(71.4786, grad_fn=<AddBackward0>)\n",
            "tensor(123.7011, grad_fn=<AddBackward0>)\n",
            "tensor(71.4160, grad_fn=<AddBackward0>)\n",
            "tensor(101.2659, grad_fn=<AddBackward0>)\n",
            "tensor(34.2538, grad_fn=<AddBackward0>)\n",
            "tensor(65.0162, grad_fn=<AddBackward0>)\n",
            "tensor(206.1894, grad_fn=<AddBackward0>)\n",
            "tensor(71.5735, grad_fn=<AddBackward0>)\n",
            "tensor(65.0671, grad_fn=<AddBackward0>)\n",
            "tensor(63.8663, grad_fn=<AddBackward0>)\n",
            "tensor(65.4395, grad_fn=<AddBackward0>)\n",
            "tensor(62.9546, grad_fn=<AddBackward0>)\n",
            "tensor(64.2363, grad_fn=<AddBackward0>)\n",
            "tensor(67.4818, grad_fn=<AddBackward0>)\n",
            "tensor(72.9308, grad_fn=<AddBackward0>)\n",
            "tensor(90.0918, grad_fn=<AddBackward0>)\n",
            "tensor(63.4531, grad_fn=<AddBackward0>)\n",
            "tensor(52.1881, grad_fn=<AddBackward0>)\n",
            "tensor(58.4760, grad_fn=<AddBackward0>)\n",
            "tensor(70.0020, grad_fn=<AddBackward0>)\n",
            "tensor(77.8224, grad_fn=<AddBackward0>)\n",
            "tensor(90.3133, grad_fn=<AddBackward0>)\n",
            "tensor(71.7164, grad_fn=<AddBackward0>)\n",
            "tensor(52.2326, grad_fn=<AddBackward0>)\n",
            "tensor(64.9530, grad_fn=<AddBackward0>)\n",
            "tensor(62.4538, grad_fn=<AddBackward0>)\n",
            "tensor(59.2441, grad_fn=<AddBackward0>)\n",
            "tensor(66.8064, grad_fn=<AddBackward0>)\n",
            "tensor(78.9610, grad_fn=<AddBackward0>)\n",
            "tensor(63.9730, grad_fn=<AddBackward0>)\n",
            "tensor(134.9273, grad_fn=<AddBackward0>)\n",
            "tensor(71.5754, grad_fn=<AddBackward0>)\n",
            "tensor(67.0636, grad_fn=<AddBackward0>)\n",
            "tensor(81.7379, grad_fn=<AddBackward0>)\n",
            "tensor(74.7354, grad_fn=<AddBackward0>)\n",
            "tensor(63.4825, grad_fn=<AddBackward0>)\n",
            "tensor(69.6852, grad_fn=<AddBackward0>)\n",
            "tensor(89.6776, grad_fn=<AddBackward0>)\n",
            "tensor(54.7362, grad_fn=<AddBackward0>)\n",
            "tensor(57.7355, grad_fn=<AddBackward0>)\n",
            "tensor(69.7128, grad_fn=<AddBackward0>)\n",
            "tensor(84.2815, grad_fn=<AddBackward0>)\n",
            "tensor(42.9854, grad_fn=<AddBackward0>)\n",
            "tensor(63.9107, grad_fn=<AddBackward0>)\n",
            "tensor(91.2842, grad_fn=<AddBackward0>)\n",
            "tensor(66.4378, grad_fn=<AddBackward0>)\n",
            "tensor(76.5521, grad_fn=<AddBackward0>)\n",
            "tensor(95.2853, grad_fn=<AddBackward0>)\n",
            "tensor(56.2306, grad_fn=<AddBackward0>)\n",
            "tensor(206.1347, grad_fn=<AddBackward0>)\n",
            "tensor(81.6032, grad_fn=<AddBackward0>)\n",
            "tensor(64.8100, grad_fn=<AddBackward0>)\n",
            "tensor(57.3400, grad_fn=<AddBackward0>)\n",
            "tensor(62.5794, grad_fn=<AddBackward0>)\n",
            "tensor(67.5319, grad_fn=<AddBackward0>)\n",
            "tensor(80.6622, grad_fn=<AddBackward0>)\n",
            "tensor(83.9387, grad_fn=<AddBackward0>)\n",
            "tensor(83.2414, grad_fn=<AddBackward0>)\n",
            "tensor(63.6863, grad_fn=<AddBackward0>)\n",
            "tensor(64.1659, grad_fn=<AddBackward0>)\n",
            "tensor(71.8281, grad_fn=<AddBackward0>)\n",
            "tensor(36.4921, grad_fn=<AddBackward0>)\n",
            "tensor(65.7316, grad_fn=<AddBackward0>)\n",
            "tensor(78.1818, grad_fn=<AddBackward0>)\n",
            "tensor(72.3947, grad_fn=<AddBackward0>)\n",
            "tensor(75.5640, grad_fn=<AddBackward0>)\n",
            "tensor(76.3321, grad_fn=<AddBackward0>)\n",
            "tensor(99.1727, grad_fn=<AddBackward0>)\n",
            "tensor(59.4111, grad_fn=<AddBackward0>)\n",
            "tensor(88.5370, grad_fn=<AddBackward0>)\n",
            "tensor(67.1726, grad_fn=<AddBackward0>)\n",
            "tensor(61.9156, grad_fn=<AddBackward0>)\n",
            "tensor(75.8703, grad_fn=<AddBackward0>)\n",
            "tensor(68.9931, grad_fn=<AddBackward0>)\n",
            "tensor(59.9445, grad_fn=<AddBackward0>)\n",
            "tensor(77.9177, grad_fn=<AddBackward0>)\n",
            "tensor(62.3991, grad_fn=<AddBackward0>)\n",
            "tensor(62.7753, grad_fn=<AddBackward0>)\n",
            "tensor(63.5906, grad_fn=<AddBackward0>)\n",
            "tensor(57.0430, grad_fn=<AddBackward0>)\n",
            "tensor(53.7445, grad_fn=<AddBackward0>)\n",
            "tensor(88.1564, grad_fn=<AddBackward0>)\n",
            "tensor(62.2213, grad_fn=<AddBackward0>)\n",
            "tensor(64.4894, grad_fn=<AddBackward0>)\n",
            "tensor(66.4072, grad_fn=<AddBackward0>)\n",
            "tensor(53.0013, grad_fn=<AddBackward0>)\n",
            "tensor(68.9565, grad_fn=<AddBackward0>)\n",
            "tensor(102.7923, grad_fn=<AddBackward0>)\n",
            "tensor(85.6471, grad_fn=<AddBackward0>)\n",
            "tensor(63.3256, grad_fn=<AddBackward0>)\n",
            "tensor(75.4712, grad_fn=<AddBackward0>)\n",
            "tensor(73.1689, grad_fn=<AddBackward0>)\n",
            "tensor(63.3079, grad_fn=<AddBackward0>)\n",
            "tensor(110.8927, grad_fn=<AddBackward0>)\n",
            "tensor(73.2449, grad_fn=<AddBackward0>)\n",
            "tensor(68.5763, grad_fn=<AddBackward0>)\n",
            "tensor(92.3354, grad_fn=<AddBackward0>)\n",
            "tensor(59.1348, grad_fn=<AddBackward0>)\n",
            "tensor(60.1342, grad_fn=<AddBackward0>)\n",
            "tensor(38.1085, grad_fn=<AddBackward0>)\n",
            "tensor(67.9070, grad_fn=<AddBackward0>)\n",
            "tensor(76.1956, grad_fn=<AddBackward0>)\n",
            "tensor(53.4974, grad_fn=<AddBackward0>)\n",
            "tensor(63.3399, grad_fn=<AddBackward0>)\n",
            "tensor(79.7919, grad_fn=<AddBackward0>)\n",
            "tensor(64.8817, grad_fn=<AddBackward0>)\n",
            "tensor(71.1642, grad_fn=<AddBackward0>)\n",
            "tensor(80.9757, grad_fn=<AddBackward0>)\n",
            "tensor(61.2863, grad_fn=<AddBackward0>)\n",
            "tensor(72.0030, grad_fn=<AddBackward0>)\n",
            "tensor(67.5380, grad_fn=<AddBackward0>)\n",
            "tensor(65.8588, grad_fn=<AddBackward0>)\n",
            "tensor(69.6539, grad_fn=<AddBackward0>)\n",
            "tensor(139.3134, grad_fn=<AddBackward0>)\n",
            "tensor(68.0830, grad_fn=<AddBackward0>)\n",
            "tensor(69.6294, grad_fn=<AddBackward0>)\n",
            "tensor(62.8702, grad_fn=<AddBackward0>)\n",
            "tensor(59.0413, grad_fn=<AddBackward0>)\n",
            "tensor(37.7345, grad_fn=<AddBackward0>)\n",
            "tensor(64.2504, grad_fn=<AddBackward0>)\n",
            "tensor(62.7730, grad_fn=<AddBackward0>)\n",
            "tensor(58.9629, grad_fn=<AddBackward0>)\n",
            "tensor(87.9885, grad_fn=<AddBackward0>)\n",
            "tensor(76.6664, grad_fn=<AddBackward0>)\n",
            "tensor(60.9220, grad_fn=<AddBackward0>)\n",
            "tensor(83.9636, grad_fn=<AddBackward0>)\n",
            "tensor(68.6548, grad_fn=<AddBackward0>)\n",
            "tensor(81.4366, grad_fn=<AddBackward0>)\n",
            "tensor(64.0674, grad_fn=<AddBackward0>)\n",
            "tensor(74.4382, grad_fn=<AddBackward0>)\n",
            "tensor(78.5802, grad_fn=<AddBackward0>)\n",
            "tensor(61.5952, grad_fn=<AddBackward0>)\n",
            "tensor(54.9509, grad_fn=<AddBackward0>)\n",
            "tensor(56.7881, grad_fn=<AddBackward0>)\n",
            "tensor(67.7602, grad_fn=<AddBackward0>)\n",
            "tensor(62.3973, grad_fn=<AddBackward0>)\n",
            "tensor(63.2032, grad_fn=<AddBackward0>)\n",
            "tensor(44.6537, grad_fn=<AddBackward0>)\n",
            "tensor(59.1588, grad_fn=<AddBackward0>)\n",
            "tensor(68.8732, grad_fn=<AddBackward0>)\n",
            "tensor(84.8706, grad_fn=<AddBackward0>)\n",
            "tensor(61.3101, grad_fn=<AddBackward0>)\n",
            "tensor(63.2639, grad_fn=<AddBackward0>)\n",
            "tensor(65.6409, grad_fn=<AddBackward0>)\n",
            "tensor(76.5683, grad_fn=<AddBackward0>)\n",
            "tensor(59.6575, grad_fn=<AddBackward0>)\n",
            "tensor(68.4788, grad_fn=<AddBackward0>)\n",
            "tensor(210.8764, grad_fn=<AddBackward0>)\n",
            "tensor(72.8080, grad_fn=<AddBackward0>)\n",
            "tensor(66.6825, grad_fn=<AddBackward0>)\n",
            "tensor(68.0298, grad_fn=<AddBackward0>)\n",
            "tensor(63.9353, grad_fn=<AddBackward0>)\n",
            "tensor(67.8730, grad_fn=<AddBackward0>)\n",
            "tensor(72.8166, grad_fn=<AddBackward0>)\n",
            "tensor(70.3459, grad_fn=<AddBackward0>)\n",
            "tensor(64.6938, grad_fn=<AddBackward0>)\n",
            "tensor(38.1850, grad_fn=<AddBackward0>)\n",
            "tensor(128.8682, grad_fn=<AddBackward0>)\n",
            "tensor(76.5182, grad_fn=<AddBackward0>)\n",
            "tensor(57.7800, grad_fn=<AddBackward0>)\n",
            "tensor(64.6792, grad_fn=<AddBackward0>)\n",
            "tensor(116.5352, grad_fn=<AddBackward0>)\n",
            "tensor(73.1066, grad_fn=<AddBackward0>)\n",
            "tensor(78.9215, grad_fn=<AddBackward0>)\n",
            "tensor(84.2926, grad_fn=<AddBackward0>)\n",
            "tensor(66.3106, grad_fn=<AddBackward0>)\n",
            "tensor(65.8048, grad_fn=<AddBackward0>)\n",
            "tensor(65.3356, grad_fn=<AddBackward0>)\n",
            "tensor(80.5648, grad_fn=<AddBackward0>)\n",
            "tensor(59.8180, grad_fn=<AddBackward0>)\n",
            "tensor(93.4556, grad_fn=<AddBackward0>)\n",
            "tensor(77.2268, grad_fn=<AddBackward0>)\n",
            "tensor(60.7647, grad_fn=<AddBackward0>)\n",
            "tensor(60.0199, grad_fn=<AddBackward0>)\n",
            "tensor(60.8694, grad_fn=<AddBackward0>)\n",
            "tensor(43.6865, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}