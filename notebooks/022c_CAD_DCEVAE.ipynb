{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disentangled Causal Effect Variational Autoencoder"
      ],
      "metadata": {
        "id": "U0xQR9NH0zm0"
      },
      "id": "U0xQR9NH0zm0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "- data/heart_disease_cleaned.csv\n",
        "\n",
        "**Outputs:**\n",
        "- DCEVEA model\n",
        "- data/fair_disease_dcevae.csv\n",
        "- data/cf_disease_dcevea.csv"
      ],
      "metadata": {
        "id": "z16vyaLs1AP5"
      },
      "id": "z16vyaLs1AP5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "zgs3GX9e1YvV"
      },
      "id": "zgs3GX9e1YvV"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = userdata.get('PROJECT_ROOT')\n",
        "except ImportError:\n",
        "  PROJECT_ROOT = '/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313eapZ0098S",
        "outputId": "5a381074-2a4c-4879-f095-73ebc93712db"
      },
      "id": "313eapZ0098S",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as utils\n",
        "import re\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "7xdN4Hk31hsu"
      },
      "id": "7xdN4Hk31hsu",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Ix6--YoyY0pT"
      },
      "id": "Ix6--YoyY0pT",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "uMw8HIuI6GvP"
      },
      "id": "uMw8HIuI6GvP"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(X_ind, X_desc, X_corr, X_sens, Y, index, batch_size=32):\n",
        "  X_ind_fact = X_ind[index]\n",
        "  X_desc_fact = X_desc[index]\n",
        "  X_corr_fact = X_corr[index]\n",
        "  X_sens_fact = X_sens[index]\n",
        "  Y_fact = Y[index]\n",
        "\n",
        "  # Permuted set for the discriminator\n",
        "  permuted_indices = np.random.permutation(X_ind_fact.shape[0])\n",
        "  X_ind_fake = X_ind[permuted_indices]\n",
        "  X_desc_fake = X_desc[permuted_indices]\n",
        "  X_corr_fake = X_corr[permuted_indices]\n",
        "  X_sens_fake = X_sens[permuted_indices]\n",
        "  Y_fake = Y[permuted_indices]\n",
        "\n",
        "  X_ind_tensor = torch.tensor(X_ind_fact, dtype=torch.float32)\n",
        "  X_desc_tensor = torch.tensor(X_desc_fact, dtype=torch.float32)\n",
        "  X_corr_tensor = torch.tensor(X_corr_fact, dtype=torch.float32)\n",
        "  X_sens_tensor = torch.tensor(X_sens_fact, dtype=torch.float32)\n",
        "  Y_tensor = torch.tensor(Y_fact, dtype=torch.float32)\n",
        "  X_ind_tensor_2 = torch.tensor(X_ind_fake, dtype=torch.float32)\n",
        "  X_desc_tensor_2 = torch.tensor(X_desc_fake, dtype=torch.float32)\n",
        "  X_corr_tensor_2 = torch.tensor(X_corr_fake, dtype=torch.float32)\n",
        "  X_sens_tensor_2 = torch.tensor(X_sens_fake, dtype=torch.float32)\n",
        "  Y_tensor_2 = torch.tensor(Y_fake, dtype=torch.float32)\n",
        "\n",
        "  dataset = utils.TensorDataset(X_ind_tensor, X_desc_tensor, X_corr_tensor, X_sens_tensor, Y_tensor,\n",
        "                                X_ind_tensor_2, X_desc_tensor_2, X_corr_tensor_2, X_sens_tensor_2, Y_tensor_2)\n",
        "  loader = utils.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def make_bucketed_loader(dataset, map, val_size=0.1, test_size=0.1, batch_size=32, seed=4):\n",
        "  '''\n",
        "    Creates train, validation and test DataLoader for the given dataset, \\\n",
        "    separating features into independent, sensitive, descendant, and correlated features.\n",
        "\n",
        "    Input:\n",
        "      - dataset: a pandas DataFrame\n",
        "      - map: a dictionary mapping feature names to buckets\n",
        "      - val_size: the proportion of the dataset to use for validation\n",
        "      - test_size: the proportion of the dataset to use for testing\n",
        "      - batch_size: the batch size for the DataLoader\n",
        "      - seed: a seed for the random number generator\n",
        "\n",
        "    Output:\n",
        "      - train_loader: Training DataLoader\n",
        "      - val_loader: Validation DataLoader\n",
        "      - test_loader: Testing DataLoader\n",
        "      - ind_types: a list of the data types of the independent features\n",
        "      - desc_types: a list of the data types of the descendant features\n",
        "      - corr_types: a list of the data types of the correlated features\n",
        "      - sens_type: the data type of the sensitive feature\n",
        "  '''\n",
        "  np.random.seed(seed=seed)\n",
        "\n",
        "  ## BUCKET DATASET\n",
        "  # Independent, Descendant, Correlated features and Sensitive attributes\n",
        "  col_ind = []\n",
        "  for feature in map['ind']:\n",
        "    col_ind.append(feature['name'])\n",
        "  X_ind = dataset[col_ind].to_numpy()\n",
        "\n",
        "\n",
        "  col_desc = []\n",
        "  for feature in map['desc']:\n",
        "    col_desc.append(feature['name'])\n",
        "  X_desc = dataset[col_desc].to_numpy()\n",
        "\n",
        "  col_corr = []\n",
        "  for feature in map['corr']:\n",
        "    col_corr.append(feature['name'])\n",
        "  X_corr = dataset[col_corr].to_numpy()\n",
        "\n",
        "  col_sens = []\n",
        "  for feature in map['sens']:\n",
        "    col_sens.append(feature['name'])\n",
        "  X_sens = dataset[col_sens].to_numpy()\n",
        "\n",
        "  # Target\n",
        "  Y = dataset[map['target']['name']].to_numpy().reshape(-1, 1)\n",
        "\n",
        "  ## TRAIN-VAL-TRAIN SPLIT\n",
        "  N = X_ind.shape[0]\n",
        "  shuffled_indices = np.random.permutation(N)\n",
        "  val_count = int(N * val_size)\n",
        "  test_count = int(N * test_size)\n",
        "  val_index = shuffled_indices[:val_count]\n",
        "  test_index = shuffled_indices[val_count:val_count+test_count]\n",
        "  train_index = shuffled_indices[val_count+test_count:]\n",
        "\n",
        "  # Training loader\n",
        "  train_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, train_index, batch_size)\n",
        "\n",
        "  # Validation loader\n",
        "  val_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, val_index, batch_size)\n",
        "\n",
        "  # Test loader\n",
        "  test_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, test_index, batch_size)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "tSQZZV156LC9"
      },
      "id": "tSQZZV156LC9",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCEVEA Architecture"
      ],
      "metadata": {
        "id": "ixf1y-38cyV_"
      },
      "id": "ixf1y-38cyV_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network architecture\n",
        "\n",
        "### Network objectives\n",
        "\n",
        "The DCEVAE network uses a min-max core objective structure to minimize the negative Evidence Lower Bound (ELBO) and Total Correlation loss ($\\mathcal{L}_{TC}$), while maximising the objective of the discriminator ($M_D$):\n",
        "\n",
        "$$min_{\\theta, \\phi}\\mathcal{L}_{DCEVAE} := - \\mathcal{M}_{ELBO} + \\beta_{TC}\\mathcal{L}_{TC}$$\n",
        "\n",
        "$$max_{\\psi}\\mathcal{M}_D$$\n",
        "\n",
        "- θ and ϕ are the paramaters for the decoder $q_{\\theta}$ and encoder $p_{\\phi}$ networks respectively\n",
        "- ψ represents the parameters of the discriminator network $D_{\\psi}$\n",
        "- $\\beta_{TC}$ is the Total Correlation scaling parameter\n",
        "\n",
        "#### ELBO\n",
        "\n",
        "Measures how well the decoder reconstructs the inputs (Expected Log-Likelihood, to be maximised) and how close the encoder's distribution of the latent variable is to a standard prior distribution (KL Divergence, to be minimised)\n",
        "\n",
        "$$ELBO = \\mathbb{E}_{q_{\\phi}(u_d|s,x_i,x_d,y)q_{\\phi}(u_c|s,x_i,x_c,y)}[\\log p_{\\theta}(y|s,x_i,u_d,u_c)]+\\mathbb{E}_{q_{\\phi}(u_d|s,x_i,x_d,y)}[\\log p_{\\theta}(y|s,x_i,u_d)]+\\mathbb{E}_{q_{\\phi}(u_c|s,x_i,x_c,y)}[\\log p_{\\theta}(y|s,x_i,u_c)]-KL(q_{\\phi}(u_d|s,x_i,x_d,y)∥p(u_d))-KL(q_{\\phi}(u_c|s,x_i,x_c,y)∥ p(u_c))$$\n",
        "\n",
        "#### Disentanglement Loss\n",
        "To ensure that the latent variables are properly disentangled from the sensitive attribute, the DCEVAE model minimises the Total Correlation loss approximated by the discriminator:\n",
        "$$\\mathcal{L}_{TC} \\approx \\mathbb{E}_{q(s,u_d,u_c)}[\\log \\frac{D_{\\psi}(s,u_d,u_c)}{1-D_{\\psi}(s,u_d,u_c)}]$$\n",
        "\n",
        "while the discriminator maximises its adversary objective:\n",
        "\n",
        "$$M_D = \\mathbb{E}_{q(s,u_d,u_c)}[\\log(D_{\\psi}(s,u_d,u_c))]+\\mathbb{E}_{q(s,u_c)q(u_d)}[\\log(1-D_{\\psi}(s,u_d,u_c))]$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CQ_gxqb2amc"
      },
      "id": "0CQ_gxqb2amc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCEVAE Model Class"
      ],
      "metadata": {
        "id": "h9yLeeDC6DlE"
      },
      "id": "h9yLeeDC6DlE"
    },
    {
      "cell_type": "code",
      "source": [
        "class DCEVAE(nn.Module):\n",
        "  def __init__(self, ind_meta, desc_meta, corr_meta, sens_meta, args):\n",
        "    super(DCEVAE, self).__init__()\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    self.ind_meta = ind_meta\n",
        "    self.desc_meta = desc_meta\n",
        "    self.corr_meta = corr_meta\n",
        "    self.sens_meta = sens_meta\n",
        "\n",
        "    self.embedding_dim = 3\n",
        "    self.embeddings = nn.ModuleDict()\n",
        "\n",
        "    def get_bucket_dim(bucket_meta):\n",
        "      '''\n",
        "        Returns the total dimension of the feature bucket\n",
        "      '''\n",
        "      total_dim = 0\n",
        "      for feature in bucket_meta:\n",
        "        if feature['type'] == 'categorical':\n",
        "          self.embeddings[feature['name']] = nn.Embedding(feature['n'], self.embedding_dim)\n",
        "          total_dim += self.embedding_dim\n",
        "        else:\n",
        "          total_dim += 1\n",
        "      return total_dim\n",
        "\n",
        "    self.ind_dim = get_bucket_dim(ind_meta)\n",
        "    self.desc_dim = get_bucket_dim(desc_meta)\n",
        "    self.corr_dim = get_bucket_dim(corr_meta)\n",
        "    self.sens_dim = get_bucket_dim(sens_meta)\n",
        "    self.target_dim = 1\n",
        "\n",
        "    self.args = args\n",
        "    self.device = args.device\n",
        "    self.uc_dim = args.uc_dim\n",
        "    self.ud_dim = args.ud_dim\n",
        "    self.u_dim = self.uc_dim + self.ud_dim\n",
        "    self.h_dim = args.h_dim\n",
        "\n",
        "    # Activation function\n",
        "    if (args.act_fn == 'relu'):\n",
        "      self.act_fn = nn.LeakyReLU()\n",
        "    elif (args.act_fn == 'tanh'):\n",
        "      self.act_fn = nn.Tanh()\n",
        "\n",
        "    # Encoder (X_ind, X_corr, X_sens, Y) -> U_c\n",
        "    input_dim = self.ind_dim + self.corr_dim + self.sens_dim + self.target_dim\n",
        "    self.encoder_corr = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "    self.logvar_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "\n",
        "    # Encoder (X_ind, X_desc, X_sens, Y) -> U_d\n",
        "    input_dim = self.ind_dim + self.desc_dim + self.sens_dim + self.target_dim\n",
        "    self.encoder_desc = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "    self.logvar_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "\n",
        "    # Decoder (U_c, X_ind) -> X_corr\n",
        "    self.decoder_corr = nn.ModuleDict()\n",
        "    for feature in corr_meta:\n",
        "      out_dim = 1 if feature['type'] != 'categorical' else feature.get('n')\n",
        "      self.decoder_corr[feature['name']] = nn.Sequential(\n",
        "          nn.Linear(self.uc_dim + self.ind_dim, self.h_dim),\n",
        "          self.act_fn,\n",
        "          nn.Linear(self.h_dim, out_dim)\n",
        "      )\n",
        "\n",
        "    # Decoder (U_desc, X_ind, X_sens) -> X_desc\n",
        "    self.decoder_desc = nn.ModuleDict()\n",
        "    for feature in desc_meta:\n",
        "      out_dim = 1 if feature['type'] != 'categorical' else feature.get('n')\n",
        "      self.decoder_desc[feature['name']] = nn.Sequential(\n",
        "          nn.Linear(self.ud_dim + self.ind_dim + self.sens_dim , self.h_dim),\n",
        "          self.act_fn,\n",
        "          nn.Linear(self.h_dim, out_dim)\n",
        "      )\n",
        "\n",
        "    # Decoder (U_desc, U_corr, X_ind, X_sens) -> Y\n",
        "    self.decoder_target = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + self.ind_dim + self.sens_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.target_dim)\n",
        "    )\n",
        "\n",
        "    # Discriminator\n",
        "    self.discriminator = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + self.sens_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, 2)\n",
        "    )\n",
        "\n",
        "    self.init_params()\n",
        "\n",
        "  def init_params(self):\n",
        "    '''\n",
        "      Initialise the network parameters.\n",
        "    '''\n",
        "    main_act_fn = \"tanh\" if isinstance(self.act_fn, nn.Tanh) else \"relu\"\n",
        "\n",
        "    for name, module in self.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "        if \"discriminator\" in name:\n",
        "          # Kaiming normal for the LeakyReLU in the Discriminator\n",
        "          nn.init.kaiming_normal(module.weight, a=0.2, nonlinearity=\"leaky_relu\")\n",
        "        else:\n",
        "          # Xavier normal for Tanh, Kaiming normal for LeakyReLU in the encoders/decoders\n",
        "          if main_act_fn == \"tanh\":\n",
        "            nn.init.xavier_normal(module.weight)\n",
        "          else:\n",
        "            nn.init.kaiming_normal(module.weight, a=0.01, nonlinearity=\"leaky_relu\")\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.constant_(module.bias, 0)\n",
        "\n",
        "  def discriminate(self, v):\n",
        "    '''\n",
        "      Discriminator forward pass\n",
        "    '''\n",
        "    return self.discriminator(v).squeeze()\n",
        "\n",
        "  def _process_features(self, x, x_meta):\n",
        "    '''\n",
        "      Processes the input features by applying embeddings to the categorical columns\n",
        "    '''\n",
        "    processed = []\n",
        "    for i, feature in enumerate(x_meta):\n",
        "      col = x[:, i]\n",
        "      if feature['type'] == 'categorical':\n",
        "        processed.append(self.embeddings[feature['name']](col.long()))\n",
        "      else:\n",
        "        processed.append(col.unsqueeze(1))\n",
        "    return torch.cat(processed, dim=1)\n",
        "\n",
        "  def encode(self, x_ind, x_desc, x_corr, x_sens, y):\n",
        "    '''\n",
        "      Encoders forward pass\n",
        "    '''\n",
        "    # Process raw tensors\n",
        "    x_ind = self._process_features(x_ind, self.ind_meta)\n",
        "    x_desc = self._process_features(x_desc, self.desc_meta)\n",
        "    x_corr = self._process_features(x_corr, self.corr_meta)\n",
        "    x_sens = self._process_features(x_sens, self.sens_meta)\n",
        "\n",
        "    # Correlated path encoder\n",
        "    input_corr = torch.cat((x_ind, x_desc, x_sens, y), dim=1)\n",
        "    h_corr = self.encoder_corr(input_corr)\n",
        "    mu_corr = self.mu_corr(h_corr)\n",
        "    logvar_corr = self.logvar_corr(h_corr)\n",
        "\n",
        "    # Descendant path encoder\n",
        "    input_desc = torch.cat((x_ind, x_desc, x_sens, y), dim=1)\n",
        "    h_desc = self.encoder_desc(input_desc)\n",
        "    mu_desc = self.mu_desc(h_desc)\n",
        "    logvar_desc = self.logvar_desc(h_desc)\n",
        "\n",
        "    return mu_corr, logvar_corr, mu_desc, logvar_desc\n",
        "\n",
        "  def decode_feature(self, u_desc, u_corr, x_ind, x_sens):\n",
        "    '''\n",
        "      Decoders forward pass\n",
        "    '''\n",
        "    # Correlated path\n",
        "    input_corr = torch.cat((u_corr, x_ind), dim=1)\n",
        "    x_corr_pred = {feature['name']: self.decoder_corr[feature['name']](input_corr)\\\n",
        "                   for feature in self.corr_meta}\n",
        "\n",
        "    # Descendant path\n",
        "    input_desc = torch.cat((u_desc, x_ind, x_sens), dim=1)\n",
        "    x_desc_pred = {feature['name']: self.decoder_desc[feature['name']](input_desc)\\\n",
        "                   for feature in self.desc_meta}\n",
        "\n",
        "    # Target\n",
        "    input_target = torch.cat((u_desc, u_corr, x_ind, x_sens), dim=1)\n",
        "    y_pred = self.decoder_target(input_target)\n",
        "\n",
        "    # Counterfactual\n",
        "    x_sens_cf = 1 - x_sens\n",
        "    input_desc_cf = torch.cat((u_desc, x_ind, x_sens_cf), dim=1)\n",
        "    x_desc_cf = {feature['name']: self.decoder_desc[feature['name']](input_desc_cf)\\\n",
        "                   for feature in self.desc_meta}\n",
        "    input_target_cf = torch.cat((u_desc, u_corr, x_ind, x_sens_cf), dim=1)\n",
        "    y_cf = self.decoder_target(input_target_cf)\n",
        "\n",
        "    return x_corr_pred, x_desc_pred, y_pred, x_desc_cf, y_cf\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    '''\n",
        "      Reparameterisation trick\n",
        "    '''\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std).to(self.device)\n",
        "    return mu + eps * std\n",
        "\n",
        "  def reconstruction_loss(self, x_ind, x_desc, x_corr, x_sens, y):\n",
        "    batch_size = self.args.batch_size\n",
        "\n",
        "    # Encode\n",
        "    mu_corr, logvar_corr, mu_desc, logvar_desc = self.encode(\n",
        "        x_ind, x_desc, x_corr, x_sens, y)\n",
        "\n",
        "    # Reparamaterise\n",
        "    u_corr = self.reparameterize(mu_corr, logvar_corr)\n",
        "    u_desc = self.reparameterize(mu_desc, logvar_desc)\n",
        "\n",
        "    # Decode\n",
        "\n",
        "    # Reconstruction Loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iNE0jzfR1ygY"
      },
      "id": "iNE0jzfR1ygY",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "LFHIW0ml12n3"
      },
      "id": "LFHIW0ml12n3"
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease = pd.read_csv(PROJECT_ROOT + '/data/heart_disease_cleaned.csv',\n",
        "                            dtype={'cp':int, 'ang':int, 'ecg':int, 'fbs':int, 'slope':int})\n",
        "\n",
        "feature_mapping = {\n",
        "    'ind': [{'name':'age','type':'continuous'}], # Features independent of the protected attribute and unconfounded\n",
        "    'sens': [{'name':'sex','type':'binary'}], # Sensitive attribute\n",
        "    'desc': [\n",
        "        {'name':'ang','type':'binary'},\n",
        "        {'name':'cp','type':'categorical', 'n':4},\n",
        "        {'name':'ecg','type':'categorical','n':3}], # Features descendant of the protected attribute\n",
        "    'corr': [\n",
        "        {'name':'bp','type':'continuous'},\n",
        "        {'name':'chol','type':'continuous'},\n",
        "        {'name':'mhr','type':'continuous'},\n",
        "        {'name':'st','type':'continuous'},\n",
        "        {'name':'fbs','type':'binary'},\n",
        "        {'name':'slope','type':'categorical', 'n':3}], # Features correlated with the protected attribute\n",
        "    'target': {'name':'cvd','type':'binary'} # Target outcome\n",
        "}\n",
        "\n",
        "# Bucketed data loaders for training , validation, and test\n",
        "train_loader, val_loader, test_loader = make_bucketed_loader(heart_disease, feature_mapping)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "et_WZlZs12Hc"
      },
      "id": "et_WZlZs12Hc",
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}