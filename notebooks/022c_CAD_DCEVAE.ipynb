{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disentangled Causal Effect Variational Autoencoder"
      ],
      "metadata": {
        "id": "U0xQR9NH0zm0"
      },
      "id": "U0xQR9NH0zm0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "- data/heart_disease_cleaned.csv\n",
        "\n",
        "**Outputs:**\n",
        "- DCEVEA model\n",
        "- data/fair_disease_dcevae.csv\n",
        "- data/cf_disease_dcevea.csv"
      ],
      "metadata": {
        "id": "z16vyaLs1AP5"
      },
      "id": "z16vyaLs1AP5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "zgs3GX9e1YvV"
      },
      "id": "zgs3GX9e1YvV"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = userdata.get('PROJECT_ROOT')\n",
        "except ImportError:\n",
        "  PROJECT_ROOT = '/'\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313eapZ0098S",
        "outputId": "88e95a9e-e250-4cca-a7b6-3ffeb2fac6f9"
      },
      "id": "313eapZ0098S",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as utils\n",
        "import re\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "7xdN4Hk31hsu"
      },
      "id": "7xdN4Hk31hsu",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Ix6--YoyY0pT"
      },
      "id": "Ix6--YoyY0pT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes and functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mDGd1cUy1hOy"
      },
      "id": "mDGd1cUy1hOy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "uMw8HIuI6GvP"
      },
      "id": "uMw8HIuI6GvP"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(X_ind, X_desc, X_corr, X_sens, Y, index, batch_size=32):\n",
        "  X_ind_fact = X_ind[index]\n",
        "  X_desc_fact = X_desc[index]\n",
        "  X_corr_fact = X_corr[index]\n",
        "  X_sens_fact = X_sens[index]\n",
        "  Y_fact = Y[index]\n",
        "\n",
        "  # Permuted set for the discriminator\n",
        "  permuted_indices = np.random.permutation(X_ind_fact.shape[0])\n",
        "  X_ind_fake = X_ind[permuted_indices]\n",
        "  X_desc_fake = X_desc[permuted_indices]\n",
        "  X_corr_fake = X_corr[permuted_indices]\n",
        "  X_sens_fake = X_sens[permuted_indices]\n",
        "  Y_fake = Y[permuted_indices]\n",
        "\n",
        "  X_ind_tensor = torch.tensor(X_ind_fact, dtype=torch.float32)\n",
        "  X_desc_tensor = torch.tensor(X_desc_fact, dtype=torch.float32)\n",
        "  X_corr_tensor = torch.tensor(X_corr_fact, dtype=torch.float32)\n",
        "  X_sens_tensor = torch.tensor(X_sens_fact, dtype=torch.float32)\n",
        "  Y_tensor = torch.tensor(Y_fact, dtype=torch.float32)\n",
        "  X_ind_tensor_2 = torch.tensor(X_ind_fake, dtype=torch.float32)\n",
        "  X_desc_tensor_2 = torch.tensor(X_desc_fake, dtype=torch.float32)\n",
        "  X_corr_tensor_2 = torch.tensor(X_corr_fake, dtype=torch.float32)\n",
        "  X_sens_tensor_2 = torch.tensor(X_sens_fake, dtype=torch.float32)\n",
        "  Y_tensor_2 = torch.tensor(Y_fake, dtype=torch.float32)\n",
        "\n",
        "  dataset = utils.TensorDataset(X_ind_tensor, X_desc_tensor, X_corr_tensor, X_sens_tensor, Y_tensor,\n",
        "                                X_ind_tensor_2, X_desc_tensor_2, X_corr_tensor_2, X_sens_tensor_2, Y_tensor_2)\n",
        "  loader = utils.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def make_bucketed_loader(dataset, map, val_size=0.1, test_size=0.1, batch_size=32, seed=4):\n",
        "  '''\n",
        "    Creates train, validation and test DataLoader for the given dataset, \\\n",
        "    separating features into independent, sensitive, descendant, and correlated features.\n",
        "\n",
        "    Input:\n",
        "      - dataset: a pandas DataFrame\n",
        "      - map: a dictionary mapping feature names to buckets\n",
        "      - val_size: the proportion of the dataset to use for validation\n",
        "      - test_size: the proportion of the dataset to use for testing\n",
        "      - batch_size: the batch size for the DataLoader\n",
        "      - seed: a seed for the random number generator\n",
        "\n",
        "    Output:\n",
        "      - train_loader: Training DataLoader\n",
        "      - val_loader: Validation DataLoader\n",
        "      - test_loader: Testing DataLoader\n",
        "  '''\n",
        "  np.random.seed(seed=seed)\n",
        "\n",
        "  ## BUCKET DATASET\n",
        "  # Independent, Descendant, Correlated features\n",
        "  r_ind = re.compile(f'{\"|\".join(map['ind'])}')\n",
        "  X_ind = dataset.filter(regex=r_ind).to_numpy()\n",
        "  r_desc = re.compile(f'{\"|\".join(map['desc'])}')\n",
        "  X_desc = dataset.filter(regex=r_desc).to_numpy()\n",
        "  r_corr = re.compile(f'{\"|\".join(map['corr'])}')\n",
        "  X_corr = dataset.filter(regex=r_corr).to_numpy()\n",
        "\n",
        "  # Sensitive attribute and Target\n",
        "  X_sens = dataset[map['sens']].to_numpy().reshape(-1, 1)\n",
        "  Y = dataset[map['target']].to_numpy().reshape(-1, 1)\n",
        "\n",
        "  ## TRAIN-VAL-TRAIN SPLIT\n",
        "  N = X_ind.shape[0]\n",
        "  shuffled_indices = np.random.permutation(N)\n",
        "  val_count = int(N * val_size)\n",
        "  test_count = int(N * test_size)\n",
        "  val_index = shuffled_indices[:val_count]\n",
        "  test_index = shuffled_indices[val_count:val_count+test_count]\n",
        "  train_index = shuffled_indices[val_count+test_count:]\n",
        "\n",
        "  # Training loader\n",
        "  train_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, train_index, batch_size)\n",
        "\n",
        "  # Validation loader\n",
        "  val_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, val_index, batch_size)\n",
        "\n",
        "  # Test loader\n",
        "  test_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, test_index, batch_size)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "tSQZZV156LC9"
      },
      "id": "tSQZZV156LC9",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCEVAE Model"
      ],
      "metadata": {
        "id": "h9yLeeDC6DlE"
      },
      "id": "h9yLeeDC6DlE"
    },
    {
      "cell_type": "code",
      "source": [
        "class DCEVAE(nn.Module):\n",
        "  def __init__(self, ind_dim, corr_dim, desc_dim, sens_dim, target_dim, args):\n",
        "    super(DCEVAE, self).__init__()\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    self.ind_dim = ind_dim\n",
        "    self.corr_dim = corr_dim\n",
        "    self.desc_dim = desc_dim\n",
        "    self.sens_dim = sens_dim\n",
        "    self.target_dim = target_dim\n",
        "    self.args = args\n",
        "    self.device = args.device\n",
        "    self.uc_dim = args.uc_dim\n",
        "    self.ud_dim = args.ud_dim\n",
        "    self.u_dim = self.uc_dim + self.ud_dim\n",
        "    self.h_dim = args.h_dim\n",
        "\n",
        "    # Activation function\n",
        "    if (args.act_fn == 'relu'):\n",
        "      self.act_fn = nn.LeakyReLU()\n",
        "    elif (args.act_fn == 'tanh'):\n",
        "      self.act_fn = nn.Tanh()\n",
        "\n",
        "    # Encoder (X_ind, X_corr, X_sens, Y) -> U_c\n",
        "    input_dim = ind_dim + corr_dim + sens_dim + target_dim\n",
        "    self.encoder_corr = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "    self.logvar_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "\n",
        "    # Encoder (X_ind, X_desc, X_sens, Y) -> U_d\n",
        "    input_dim = ind_dim + desc_dim + sens_dim + target_dim\n",
        "    self.encoder_desc = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "    self.logvar_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "\n",
        "    # Decoder (U_c, X_ind) -> X_corr\n",
        "    self.decoder_corr = nn.Sequential(\n",
        "        nn.Linear(self.uc_dim + ind_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.corr_dim)\n",
        "    )\n",
        "\n",
        "    # Decoder (U_desc, X_sens, X_ind) -> X_desc\n",
        "    self.decoder_desc = nn.Sequential(\n",
        "        nn.Linear(self.ud_dim + sens_dim + ind_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.desc_dim)\n",
        "    )\n",
        "\n",
        "    # Decoder (U_corr, U_desc, X_ind, X_sens) -> Y\n",
        "    self.decoder_target = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + ind_dim + sens_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.target_dim)\n",
        "    )\n",
        "\n",
        "    # Discriminator\n",
        "    self.discriminator = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + sens_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, 2)\n",
        "    )\n",
        "\n",
        "    self.init_params()\n",
        "\n",
        "  def init_params(self):\n",
        "    main_act_fn = \"tanh\" if isinstance(self.act_fn, nn.Tanh) else \"relu\"\n",
        "\n",
        "    for name, module in self.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "        if \"discriminator\" in name:\n",
        "          # Kaiming normal for the LeakyReLU in the Discriminator\n",
        "          nn.init.kaiming_normal(module.weight, a=0.2, nonlinearity=\"leaky_relu\")\n",
        "        else:\n",
        "          # Xavier normal for Tanh, Kaiming normal for LeakyReLU in the encoders/decoders\n",
        "          if main_act_fn == \"tanh\":\n",
        "            nn.init.xavier_normal(module.weight)\n",
        "          else:\n",
        "            nn.init.kaiming_normal(module.weight, a=0.01, nonlinearity=\"leaky_relu\")\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.constant_(module.bias, 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "iNE0jzfR1ygY"
      },
      "id": "iNE0jzfR1ygY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "LFHIW0ml12n3"
      },
      "id": "LFHIW0ml12n3"
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease = pd.read_csv(PROJECT_ROOT + '/data/heart_disease_cleaned.csv')\n",
        "\n",
        "# Hot-on encoding for categorical features\n",
        "heart_disease_encoded = pd.get_dummies(heart_disease, columns=['cp','ecg','slope'], drop_first=True, dtype=int)\n",
        "\n",
        "feature_mapping = {\n",
        "    'ind': ['age'], # Features independent of the protected attribute and unconfounded\n",
        "    'sens': 'sex', # Sensitive attribute\n",
        "    'desc': ['cp', 'ecg', 'ang'], # Features descendant of the protected attribute\n",
        "    'corr': ['bp', 'chol', 'fbs', 'mhr', 'st', 'slope'], # Features correlated with the protected attribute\n",
        "    'target': 'cvd' # Target outcome\n",
        "}\n",
        "\n",
        "# Bucketed data loaders for training , validation, and test\n",
        "train_loader, val_loader, test_loader = make_bucketed_loader(heart_disease_encoded, feature_mapping)"
      ],
      "metadata": {
        "id": "et_WZlZs12Hc"
      },
      "id": "et_WZlZs12Hc",
      "execution_count": 94,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}