{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disentangled Causal Effect Variational Autoencoder"
      ],
      "metadata": {
        "id": "U0xQR9NH0zm0"
      },
      "id": "U0xQR9NH0zm0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "- data/heart_disease_cleaned.csv\n",
        "\n",
        "**Outputs:**\n",
        "- DCEVEA model\n",
        "- data/fair_disease_dcevae.csv\n",
        "- data/cf_disease_dcevea.csv"
      ],
      "metadata": {
        "id": "z16vyaLs1AP5"
      },
      "id": "z16vyaLs1AP5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "zgs3GX9e1YvV"
      },
      "id": "zgs3GX9e1YvV"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = userdata.get('PROJECT_ROOT')\n",
        "except ImportError:\n",
        "  PROJECT_ROOT = '/'\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313eapZ0098S",
        "outputId": "b7795944-46a7-47b2-8c7c-2a54f1b938df"
      },
      "id": "313eapZ0098S",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as utils\n",
        "import re\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "7xdN4Hk31hsu"
      },
      "id": "7xdN4Hk31hsu",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes and functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mDGd1cUy1hOy"
      },
      "id": "mDGd1cUy1hOy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCEVAE Model"
      ],
      "metadata": {
        "id": "h9yLeeDC6DlE"
      },
      "id": "h9yLeeDC6DlE"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNE0jzfR1ygY"
      },
      "id": "iNE0jzfR1ygY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "uMw8HIuI6GvP"
      },
      "id": "uMw8HIuI6GvP"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KazNO08AZ03Y"
      },
      "id": "KazNO08AZ03Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(X_ind, X_desc, X_corr, X_sens, Y, index, batch_size=32):\n",
        "  X_ind_fact = X_ind[index]\n",
        "  X_desc_fact = X_desc[index]\n",
        "  X_corr_fact = X_corr[index]\n",
        "  X_sens_fact = X_sens[index]\n",
        "  Y_fact = Y[index]\n",
        "\n",
        "  permuted_indices = np.random.permutation(X_ind_fact.shape[0])\n",
        "  X_ind_fake = X_ind[permuted_indices]\n",
        "  X_desc_fake = X_desc[permuted_indices]\n",
        "  X_corr_fake = X_corr[permuted_indices]\n",
        "  X_sens_fake = X_sens[permuted_indices]\n",
        "  Y_fake = Y[permuted_indices]\n",
        "\n",
        "  X_ind_tensor = torch.tensor(X_ind_fact, dtype=torch.float32)\n",
        "  X_desc_tensor = torch.tensor(X_desc_fact, dtype=torch.float32)\n",
        "  X_corr_tensor = torch.tensor(X_corr_fact, dtype=torch.float32)\n",
        "  X_sens_tensor = torch.tensor(X_sens_fact, dtype=torch.float32)\n",
        "  Y_tensor = torch.tensor(Y_fact, dtype=torch.float32)\n",
        "  X_ind_tensor_2 = torch.tensor(X_ind_fake, dtype=torch.float32)\n",
        "  X_desc_tensor_2 = torch.tensor(X_desc_fake, dtype=torch.float32)\n",
        "  X_corr_tensor_2 = torch.tensor(X_corr_fake, dtype=torch.float32)\n",
        "  X_sens_tensor_2 = torch.tensor(X_sens_fake, dtype=torch.float32)\n",
        "  Y_tensor_2 = torch.tensor(Y_fake, dtype=torch.float32)\n",
        "\n",
        "  dataset = utils.TensorDataset(X_ind_tensor, X_desc_tensor, X_corr_tensor, X_sens_tensor, Y_tensor,\n",
        "                                X_ind_tensor_2, X_desc_tensor_2, X_corr_tensor_2, X_sens_tensor_2, Y_tensor_2)\n",
        "  loader = utils.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def make_bucketed_loader(dataset, map, val_size=0.1, test_size=0.1, batch_size=32, seed=4):\n",
        "  '''\n",
        "    Creates train, validation and test DataLoader for the given dataset, \\\n",
        "    separating features into independent, sensitive, descendant, and correlated features.\n",
        "\n",
        "    Input:\n",
        "      - dataset: a pandas DataFrame\n",
        "      - map: a dictionary mapping feature names to buckets\n",
        "      - val_size: the proportion of the dataset to use for validation\n",
        "      - test_size: the proportion of the dataset to use for testing\n",
        "      - batch_size: the batch size for the DataLoader\n",
        "      - seed: a seed for the random number generator\n",
        "\n",
        "    Output:\n",
        "      - train_loader: Training DataLoader\n",
        "      - val_loader: Validation DataLoader\n",
        "      - test_loader: Testing DataLoader\n",
        "  '''\n",
        "  np.random.seed(seed=seed)\n",
        "\n",
        "  ## BUCKET DATASET\n",
        "  # Independent, Descendant, Correlated features\n",
        "  r_ind = re.compile(f'{\"|\".join(map['ind'])}')\n",
        "  X_ind = dataset.filter(regex=r_ind).to_numpy()\n",
        "  r_desc = re.compile(f'{\"|\".join(map['desc'])}')\n",
        "  X_desc = dataset.filter(regex=r_desc).to_numpy()\n",
        "  r_corr = re.compile(f'{\"|\".join(map['corr'])}')\n",
        "  X_corr = dataset.filter(regex=r_corr).to_numpy()\n",
        "\n",
        "  # Sensitive attribute and Target\n",
        "  X_sens = dataset[map['sens']].to_numpy().reshape(-1, 1)\n",
        "  Y = dataset[map['target']].to_numpy().reshape(-1, 1)\n",
        "\n",
        "  ## TRAIN-VAL-TRAIN SPLIT\n",
        "  N = X_ind.shape[0]\n",
        "  shuffled_indices = np.random.permutation(N)\n",
        "  val_count = int(N * val_size)\n",
        "  test_count = int(N * test_size)\n",
        "  val_index = shuffled_indices[:val_count]\n",
        "  test_index = shuffled_indices[val_count:val_count+test_count]\n",
        "  train_index = shuffled_indices[val_count+test_count:]\n",
        "\n",
        "  # Training loader\n",
        "  train_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, train_index, batch_size)\n",
        "\n",
        "  # Validation loader\n",
        "  val_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, val_index, batch_size)\n",
        "\n",
        "  # Test loader\n",
        "  test_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, test_index, batch_size)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "tSQZZV156LC9"
      },
      "id": "tSQZZV156LC9",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "LFHIW0ml12n3"
      },
      "id": "LFHIW0ml12n3"
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease = pd.read_csv(PROJECT_ROOT + '/data/heart_disease_cleaned.csv')\n",
        "\n",
        "# Hot-on encoding for categorical features\n",
        "heart_disease_encoded = pd.get_dummies(heart_disease, columns=['cp','ecg','slope'], drop_first=True, dtype=int)\n",
        "\n",
        "feature_mapping = {\n",
        "    'ind': ['age'], # Features independent of the protected attribute and unconfounded\n",
        "    'sens': 'sex', # Sensitive attribute\n",
        "    'desc': ['cp', 'ecg', 'ang'], # Features descendant of the protected attribute\n",
        "    'corr': ['bp', 'chol', 'fbs', 'mhr', 'st', 'slope'], # Features correlated with the protected attribute\n",
        "    'target': 'cvd' # Target outcome\n",
        "}\n",
        "\n",
        "# Bucketed data loaders for training , validation, and test\n",
        "train_loader, val_loader, test_loader = make_bucketed_loader(heart_disease_encoded, feature_mapping)"
      ],
      "metadata": {
        "id": "et_WZlZs12Hc"
      },
      "id": "et_WZlZs12Hc",
      "execution_count": 94,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}