{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disentangled Causal Effect Variational Autoencoder"
      ],
      "metadata": {
        "id": "U0xQR9NH0zm0"
      },
      "id": "U0xQR9NH0zm0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "- data/heart_disease_cleaned.csv\n",
        "\n",
        "**Outputs:**\n",
        "- DCEVEA model\n",
        "- data/fair_disease_dcevae.csv\n",
        "- data/cf_disease_dcevea.csv"
      ],
      "metadata": {
        "id": "z16vyaLs1AP5"
      },
      "id": "z16vyaLs1AP5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "zgs3GX9e1YvV"
      },
      "id": "zgs3GX9e1YvV"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = userdata.get('PROJECT_ROOT')\n",
        "except ImportError:\n",
        "  PROJECT_ROOT = '/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313eapZ0098S",
        "outputId": "868a2273-3b5f-45ba-f7ff-6d454e741107"
      },
      "id": "313eapZ0098S",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as utils\n",
        "import re\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "7xdN4Hk31hsu"
      },
      "id": "7xdN4Hk31hsu",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_PATH = PROJECT_ROOT + '/models/dcevae.pt'\n",
        "SEED = 4\n",
        "BATCH_SIZE = 32\n",
        "UC_DIM = 3\n",
        "UD_DIM = 3\n",
        "H_DIM = 5\n",
        "ACT_FN = 'relu'\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "CORR_RECON_ALPHA = 1\n",
        "DESC_RECON_ALPHA = 1\n",
        "PRED_ALPHA = 1\n",
        "FAIR_BETA = 1\n",
        "TC_BETA = 1\n",
        "\n",
        "args = Namespace(\n",
        "    device=DEVICE,\n",
        "    model_path=MODEL_PATH,\n",
        "    seed=SEED,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    uc_dim=UC_DIM,\n",
        "    ud_dim=UD_DIM,\n",
        "    h_dim=H_DIM,\n",
        "    act_fn=ACT_FN,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    lr = LEARNING_RATE,\n",
        "    corr_a = CORR_RECON_ALPHA,\n",
        "    desc_a = DESC_RECON_ALPHA,\n",
        "    pred_a = PRED_ALPHA,\n",
        "    fair_b = FAIR_BETA,\n",
        "    tc_b = TC_BETA\n",
        ")"
      ],
      "metadata": {
        "id": "Ix6--YoyY0pT"
      },
      "id": "Ix6--YoyY0pT",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "uMw8HIuI6GvP"
      },
      "id": "uMw8HIuI6GvP"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(X_ind, X_desc, X_corr, X_sens, Y, index, batch_size=32):\n",
        "  X_ind_fact = X_ind[index]\n",
        "  X_desc_fact = X_desc[index]\n",
        "  X_corr_fact = X_corr[index]\n",
        "  X_sens_fact = X_sens[index]\n",
        "  Y_fact = Y[index]\n",
        "\n",
        "  # Permuted set for the discriminator\n",
        "  permuted_indices = np.random.permutation(X_ind_fact.shape[0])\n",
        "  X_ind_fake = X_ind[permuted_indices]\n",
        "  X_desc_fake = X_desc[permuted_indices]\n",
        "  X_corr_fake = X_corr[permuted_indices]\n",
        "  X_sens_fake = X_sens[permuted_indices]\n",
        "  Y_fake = Y[permuted_indices]\n",
        "\n",
        "  X_ind_tensor = torch.tensor(X_ind_fact, dtype=torch.float32)\n",
        "  X_desc_tensor = torch.tensor(X_desc_fact, dtype=torch.float32)\n",
        "  X_corr_tensor = torch.tensor(X_corr_fact, dtype=torch.float32)\n",
        "  X_sens_tensor = torch.tensor(X_sens_fact, dtype=torch.float32)\n",
        "  Y_tensor = torch.tensor(Y_fact, dtype=torch.float32)\n",
        "  X_ind_tensor_2 = torch.tensor(X_ind_fake, dtype=torch.float32)\n",
        "  X_desc_tensor_2 = torch.tensor(X_desc_fake, dtype=torch.float32)\n",
        "  X_corr_tensor_2 = torch.tensor(X_corr_fake, dtype=torch.float32)\n",
        "  X_sens_tensor_2 = torch.tensor(X_sens_fake, dtype=torch.float32)\n",
        "  Y_tensor_2 = torch.tensor(Y_fake, dtype=torch.float32)\n",
        "\n",
        "  dataset = utils.TensorDataset(X_ind_tensor, X_desc_tensor, X_corr_tensor, X_sens_tensor, Y_tensor,\n",
        "                                X_ind_tensor_2, X_desc_tensor_2, X_corr_tensor_2, X_sens_tensor_2, Y_tensor_2)\n",
        "  loader = utils.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def make_bucketed_loader(dataset, map, val_size=0.1, test_size=0.1, batch_size=32, seed=4):\n",
        "  '''\n",
        "    Creates train, validation and test DataLoader for the given dataset, \\\n",
        "    separating features into independent, sensitive, descendant, and correlated features.\n",
        "\n",
        "    Input:\n",
        "      - dataset: a pandas DataFrame\n",
        "      - map: a dictionary mapping feature names to buckets\n",
        "      - val_size: the proportion of the dataset to use for validation\n",
        "      - test_size: the proportion of the dataset to use for testing\n",
        "      - batch_size: the batch size for the DataLoader\n",
        "      - seed: a seed for the random number generator\n",
        "\n",
        "    Output:\n",
        "      - train_loader: Training DataLoader\n",
        "      - val_loader: Validation DataLoader\n",
        "      - test_loader: Testing DataLoader\n",
        "      - ind_types: a list of the data types of the independent features\n",
        "      - desc_types: a list of the data types of the descendant features\n",
        "      - corr_types: a list of the data types of the correlated features\n",
        "      - sens_type: the data type of the sensitive feature\n",
        "  '''\n",
        "  np.random.seed(seed=seed)\n",
        "\n",
        "  ## BUCKET DATASET\n",
        "  # Independent, Descendant, Correlated features and Sensitive attributes\n",
        "  col_ind = []\n",
        "  for feature in map['ind']:\n",
        "    col_ind.append(feature['name'])\n",
        "  X_ind = dataset[col_ind].to_numpy()\n",
        "\n",
        "\n",
        "  col_desc = []\n",
        "  for feature in map['desc']:\n",
        "    col_desc.append(feature['name'])\n",
        "  X_desc = dataset[col_desc].to_numpy()\n",
        "\n",
        "  col_corr = []\n",
        "  for feature in map['corr']:\n",
        "    col_corr.append(feature['name'])\n",
        "  X_corr = dataset[col_corr].to_numpy()\n",
        "\n",
        "  col_sens = []\n",
        "  for feature in map['sens']:\n",
        "    col_sens.append(feature['name'])\n",
        "  X_sens = dataset[col_sens].to_numpy()\n",
        "\n",
        "  # Target\n",
        "  Y = dataset[map['target']['name']].to_numpy().reshape(-1, 1)\n",
        "\n",
        "  ## TRAIN-VAL-TRAIN SPLIT\n",
        "  N = X_ind.shape[0]\n",
        "  shuffled_indices = np.random.permutation(N)\n",
        "  val_count = int(N * val_size)\n",
        "  test_count = int(N * test_size)\n",
        "  val_index = shuffled_indices[:val_count]\n",
        "  test_index = shuffled_indices[val_count:val_count+test_count]\n",
        "  train_index = shuffled_indices[val_count+test_count:]\n",
        "\n",
        "  # Training loader\n",
        "  train_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, train_index, batch_size)\n",
        "\n",
        "  # Validation loader\n",
        "  val_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, val_index, batch_size)\n",
        "\n",
        "  # Test loader\n",
        "  test_loader = make_loader(X_ind, X_desc, X_corr, X_sens, Y, test_index, batch_size)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "tSQZZV156LC9"
      },
      "id": "tSQZZV156LC9",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCEVEA Architecture"
      ],
      "metadata": {
        "id": "ixf1y-38cyV_"
      },
      "id": "ixf1y-38cyV_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network architecture\n",
        "\n",
        "### Network objectives\n",
        "\n",
        "The DCEVAE network uses a min-max core objective structure to minimize the negative Evidence Lower Bound (ELBO) and Total Correlation loss ($\\mathcal{L}_{TC}$), while maximising the objective of the discriminator ($M_D$):\n",
        "\n",
        "$$min_{\\theta, \\phi}\\mathcal{L}_{DCEVAE} := - \\mathcal{M}_{ELBO} + \\beta_{TC}\\mathcal{L}_{TC}$$\n",
        "\n",
        "$$max_{\\psi}\\mathcal{M}_D$$\n",
        "\n",
        "- θ and ϕ are the paramaters for the decoder $q_{\\theta}$ and encoder $p_{\\phi}$ networks respectively\n",
        "- ψ represents the parameters of the discriminator network $D_{\\psi}$\n",
        "- $\\beta_{TC}$ is the Total Correlation scaling parameter\n",
        "\n",
        "#### ELBO\n",
        "\n",
        "Measures how well the decoder reconstructs the inputs (Expected Log-Likelihood, to be maximised) and how close the encoder's distribution of the latent variable is to a standard prior distribution (KL Divergence, to be minimised)\n",
        "\n",
        "$$ELBO = \\mathbb{E}_{q_{\\phi}(u_d|s,x_i,x_d,y)q_{\\phi}(u_c|s,x_i,x_c,y)}[\\log p_{\\theta}(y|s,x_i,u_d,u_c)]+\\mathbb{E}_{q_{\\phi}(u_d|s,x_i,x_d,y)}[\\log p_{\\theta}(y|s,x_i,u_d)]+\\mathbb{E}_{q_{\\phi}(u_c|s,x_i,x_c,y)}[\\log p_{\\theta}(y|s,x_i,u_c)]-KL(q_{\\phi}(u_d|s,x_i,x_d,y)∥p(u_d))-KL(q_{\\phi}(u_c|s,x_i,x_c,y)∥ p(u_c))$$\n",
        "\n",
        "#### Disentanglement Loss\n",
        "To ensure that the latent variables are properly disentangled from the sensitive attribute, the DCEVAE model minimises the Total Correlation loss approximated by the discriminator:\n",
        "$$\\mathcal{L}_{TC} \\approx \\mathbb{E}_{q(s,u_d,u_c)}[\\log \\frac{D_{\\psi}(s,u_d,u_c)}{1-D_{\\psi}(s,u_d,u_c)}]$$\n",
        "\n",
        "while the discriminator maximises its adversary objective:\n",
        "\n",
        "$$M_D = \\mathbb{E}_{q(s,u_d,u_c)}[\\log(D_{\\psi}(s,u_d,u_c))]+\\mathbb{E}_{q(s,u_c)q(u_d)}[\\log(1-D_{\\psi}(s,u_d,u_c))]$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CQ_gxqb2amc"
      },
      "id": "0CQ_gxqb2amc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCEVAE Model Class"
      ],
      "metadata": {
        "id": "h9yLeeDC6DlE"
      },
      "id": "h9yLeeDC6DlE"
    },
    {
      "cell_type": "code",
      "source": [
        "class DCEVAE(nn.Module):\n",
        "  def __init__(self, ind_meta, desc_meta, corr_meta, sens_meta, args):\n",
        "    super(DCEVAE, self).__init__()\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    self.ind_meta = ind_meta\n",
        "    self.desc_meta = desc_meta\n",
        "    self.corr_meta = corr_meta\n",
        "    self.sens_meta = sens_meta\n",
        "\n",
        "    self.embedding_dim = 3\n",
        "    self.embeddings = nn.ModuleDict()\n",
        "\n",
        "    def get_bucket_dim(bucket_meta):\n",
        "      '''\n",
        "        Returns the total dimension of the feature bucket\n",
        "      '''\n",
        "      total_dim = 0\n",
        "      for feature in bucket_meta:\n",
        "        if feature['type'] == 'categorical':\n",
        "          self.embeddings[feature['name']] = nn.Embedding(feature['n'], self.embedding_dim)\n",
        "          total_dim += self.embedding_dim\n",
        "        else:\n",
        "          total_dim += 1\n",
        "      return total_dim\n",
        "\n",
        "    self.ind_dim = get_bucket_dim(ind_meta)\n",
        "    self.desc_dim = get_bucket_dim(desc_meta)\n",
        "    self.corr_dim = get_bucket_dim(corr_meta)\n",
        "    self.sens_dim = get_bucket_dim(sens_meta)\n",
        "    self.target_dim = 1\n",
        "\n",
        "    self.args = args\n",
        "    self.device = args.device\n",
        "    self.uc_dim = args.uc_dim\n",
        "    self.ud_dim = args.ud_dim\n",
        "    self.u_dim = self.uc_dim + self.ud_dim\n",
        "    self.h_dim = args.h_dim\n",
        "    self.batch_size = args.batch_size\n",
        "\n",
        "    # Activation function\n",
        "    if (args.act_fn == 'relu'):\n",
        "      self.act_fn = nn.LeakyReLU()\n",
        "    elif (args.act_fn == 'tanh'):\n",
        "      self.act_fn = nn.Tanh()\n",
        "\n",
        "    # Encoder (X_ind, X_corr, X_sens, Y) -> U_c\n",
        "    input_dim = self.ind_dim + self.corr_dim + self.sens_dim + self.target_dim\n",
        "    self.encoder_corr = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "    self.logvar_corr = nn.Linear(self.h_dim, self.uc_dim)\n",
        "\n",
        "    # Encoder (X_ind, X_desc, X_sens, Y) -> U_d\n",
        "    input_dim = self.ind_dim + self.desc_dim + self.sens_dim + self.target_dim\n",
        "    self.encoder_desc = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        self.act_fn\n",
        "    )\n",
        "    self.mu_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "    self.logvar_desc = nn.Linear(self.h_dim, self.ud_dim)\n",
        "\n",
        "    # Decoder (U_c, X_ind) -> X_corr\n",
        "    self.decoder_corr = nn.ModuleDict()\n",
        "    for feature in corr_meta:\n",
        "      out_dim = 1 if feature['type'] != 'categorical' else feature.get('n')\n",
        "      self.decoder_corr[feature['name']] = nn.Sequential(\n",
        "          nn.Linear(self.uc_dim + self.ind_dim, self.h_dim),\n",
        "          self.act_fn,\n",
        "          nn.Linear(self.h_dim, out_dim)\n",
        "      )\n",
        "\n",
        "    # Decoder (U_desc, X_ind, X_sens) -> X_desc\n",
        "    self.decoder_desc = nn.ModuleDict()\n",
        "    for feature in desc_meta:\n",
        "      out_dim = 1 if feature['type'] != 'categorical' else feature.get('n')\n",
        "      self.decoder_desc[feature['name']] = nn.Sequential(\n",
        "          nn.Linear(self.ud_dim + self.ind_dim + self.sens_dim , self.h_dim),\n",
        "          self.act_fn,\n",
        "          nn.Linear(self.h_dim, out_dim)\n",
        "      )\n",
        "\n",
        "    # Decoder (U_desc, U_corr, X_ind, X_sens) -> Y\n",
        "    self.decoder_target = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + self.ind_dim + self.sens_dim, self.h_dim),\n",
        "        self.act_fn,\n",
        "        nn.Linear(self.h_dim, self.target_dim)\n",
        "    )\n",
        "\n",
        "    # Discriminator\n",
        "    self.discriminator = nn.Sequential(\n",
        "        nn.Linear(self.u_dim + self.sens_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, self.h_dim),\n",
        "        nn.LeakyReLU(0.2, True),\n",
        "        nn.Linear(self.h_dim, 2)\n",
        "    )\n",
        "\n",
        "    self.init_params()\n",
        "\n",
        "  def init_params(self):\n",
        "    '''\n",
        "      Initialise the network parameters.\n",
        "    '''\n",
        "    main_act_fn = \"tanh\" if isinstance(self.act_fn, nn.Tanh) else \"relu\"\n",
        "\n",
        "    for name, module in self.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "        if \"discriminator\" in name:\n",
        "          # Kaiming normal for the LeakyReLU in the Discriminator\n",
        "          nn.init.kaiming_normal_(module.weight, a=0.2, nonlinearity=\"leaky_relu\")\n",
        "        else:\n",
        "          # Xavier normal for Tanh, Kaiming normal for LeakyReLU in the encoders/decoders\n",
        "          if main_act_fn == \"tanh\":\n",
        "            nn.init.xavier_normal(module.weight)\n",
        "          else:\n",
        "            nn.init.kaiming_normal_(module.weight, a=0.01, nonlinearity=\"leaky_relu\")\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.constant_(module.bias, 0)\n",
        "\n",
        "  def discriminate(self, v):\n",
        "    '''\n",
        "      Discriminator forward pass\n",
        "    '''\n",
        "    return self.discriminator(v).squeeze()\n",
        "\n",
        "  def _process_features(self, x, x_meta):\n",
        "    '''\n",
        "      Processes the input features by applying embeddings to the categorical columns\n",
        "    '''\n",
        "    processed = []\n",
        "    for i, feature in enumerate(x_meta):\n",
        "      col = x[:, i]\n",
        "      if feature['type'] == 'categorical':\n",
        "        processed.append(self.embeddings[feature['name']](col.long()))\n",
        "      else:\n",
        "        processed.append(col.unsqueeze(1))\n",
        "    return torch.cat(processed, dim=1)\n",
        "\n",
        "  def encode(self, x_ind, x_desc, x_corr, x_sens, y):\n",
        "    '''\n",
        "      Encoders forward pass\n",
        "    '''\n",
        "    # Process raw tensors\n",
        "    x_ind_p = self._process_features(x_ind, self.ind_meta)\n",
        "    x_desc_p = self._process_features(x_desc, self.desc_meta)\n",
        "    x_corr_p = self._process_features(x_corr, self.corr_meta)\n",
        "    x_sens_p = self._process_features(x_sens, self.sens_meta)\n",
        "\n",
        "    # Correlated path encoder\n",
        "    input_corr = torch.cat((x_ind_p, x_corr_p, x_sens_p, y), dim=1)\n",
        "    h_corr = self.encoder_corr(input_corr)\n",
        "    mu_corr = self.mu_corr(h_corr)\n",
        "    logvar_corr = self.logvar_corr(h_corr)\n",
        "\n",
        "    # Descendant path encoder\n",
        "    input_desc = torch.cat((x_ind_p, x_desc_p, x_sens_p, y), dim=1)\n",
        "    h_desc = self.encoder_desc(input_desc)\n",
        "    mu_desc = self.mu_desc(h_desc)\n",
        "    logvar_desc = self.logvar_desc(h_desc)\n",
        "\n",
        "    return mu_corr, logvar_corr, mu_desc, logvar_desc\n",
        "\n",
        "  def decode(self, u_desc, u_corr, x_ind, x_sens):\n",
        "    '''\n",
        "      Decoders forward pass\n",
        "    '''\n",
        "    # Process raw tensors\n",
        "    x_ind_p = self._process_features(x_ind, self.ind_meta)\n",
        "    x_sens_p = self._process_features(x_sens, self.sens_meta)\n",
        "\n",
        "    # Correlated path\n",
        "    input_corr = torch.cat((u_corr, x_ind_p), dim=1)\n",
        "    x_corr_pred = {feature['name']: self.decoder_corr[feature['name']](input_corr)\\\n",
        "                   for feature in self.corr_meta}\n",
        "\n",
        "    # Descendant path\n",
        "    input_desc = torch.cat((u_desc, x_ind_p, x_sens_p), dim=1)\n",
        "    x_desc_pred = {feature['name']: self.decoder_desc[feature['name']](input_desc)\\\n",
        "                   for feature in self.desc_meta}\n",
        "\n",
        "    # Target\n",
        "    input_target = torch.cat((u_desc, u_corr, x_ind_p, x_sens_p), dim=1)\n",
        "    y_pred = self.decoder_target(input_target)\n",
        "\n",
        "    # Counterfactual\n",
        "    x_sens_cf_p = self._process_features(1 - x_sens, self.sens_meta)\n",
        "    input_desc_cf = torch.cat((u_desc, x_ind_p, x_sens_cf_p), dim=1)\n",
        "    x_desc_cf = {feature['name']: self.decoder_desc[feature['name']](input_desc_cf)\\\n",
        "                   for feature in self.desc_meta}\n",
        "\n",
        "    input_target_cf = torch.cat((u_desc, u_corr, x_ind_p, x_sens_cf_p), dim=1)\n",
        "    y_cf = self.decoder_target(input_target_cf)\n",
        "\n",
        "    return x_corr_pred, x_desc_pred, y_pred, x_desc_cf, y_cf\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    '''\n",
        "      Reparameterisation trick\n",
        "    '''\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std).to(self.device)\n",
        "    return mu + eps * std\n",
        "\n",
        "  def reconstruction_loss(self, v_pred, v, v_meta):\n",
        "    '''\n",
        "      Calculates the total reconstruction loss for the given feature bucket,\\\n",
        "       matching each feature type to the correct loss function\n",
        "    '''\n",
        "    sample_size = v.size(0)\n",
        "    total_recon_L = 0\n",
        "    for i, feature in enumerate(v_meta):\n",
        "      pred = v_pred[feature['name']]\n",
        "      target = v[:, i]\n",
        "\n",
        "      if feature['type'] == 'categorical':\n",
        "        total_recon_L += nn.CrossEntropyLoss(reduction='sum')(pred, target.long())\n",
        "      elif feature['type'] == 'binary':\n",
        "        total_recon_L += nn.BCEWithLogitsLoss(reduction='sum')(pred, target.unsqueeze(1))\n",
        "      else:\n",
        "        total_recon_L += nn.MSELoss(reduction='sum')(pred, target.unsqueeze(1))\n",
        "\n",
        "    return total_recon_L / sample_size\n",
        "\n",
        "  def kl_loss(self, mu, logvar):\n",
        "    '''\n",
        "      Calculates the KL divergence for the given latent variable between the \\\n",
        "      approzimate posterior q(u|x,y,a) and the standard normal prior p(u) = N(0,I)\n",
        "    '''\n",
        "    logvar_clamped = torch.clamp(logvar, -10.0, 10.0)\n",
        "\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar_clamped - mu.pow(2) - logvar_clamped.exp())\n",
        "\n",
        "    return kl_div / mu.size(0)\n",
        "\n",
        "  def tc_loss(self, u_desc, u_corr, x_sens):\n",
        "    '''\n",
        "      Calculates the Total Correlation loss to minimise for the VAE and the \\\n",
        "      discriminator loss\n",
        "    '''\n",
        "    sample_size = u_desc.size(0)\n",
        "\n",
        "    # Process raw tensors\n",
        "    x_sens_p = self._process_features(x_sens, self.sens_meta)\n",
        "\n",
        "    # Run the discriminator on factual samples\n",
        "    input_disc = torch.cat((u_desc, u_corr, x_sens_p), dim=1)\n",
        "    disc_logits = self.discriminator(input_disc)\n",
        "\n",
        "    # Prepare permuted samples\n",
        "    permuted_indices = np.random.permutation(u_desc.size(0))\n",
        "    u_desc_permuted = u_desc[permuted_indices]\n",
        "\n",
        "    # Run the discriminator on permuted samples\n",
        "    input_disc_permuted = torch.cat((u_desc_permuted, u_corr, x_sens_p), dim=1)\n",
        "    disc_logits_permuted = self.discriminator(input_disc_permuted)\n",
        "\n",
        "    # Calculate the TC loss to minimise for the VAE\n",
        "    tc_L = (disc_logits[:, 1] - disc_logits[:, 0]).mean()\n",
        "\n",
        "    # Calculate the discriminator loss\n",
        "    target_real = torch.ones(sample_size, dtype=torch.long).to(self.device)\n",
        "    target_fake = torch.zeros(sample_size, dtype=torch.long).to(self.device)\n",
        "    disc_L = nn.CrossEntropyLoss()(disc_logits, target_real) + nn.CrossEntropyLoss()(disc_logits_permuted, target_fake)\n",
        "\n",
        "    return tc_L, disc_L\n",
        "\n",
        "  def fair_loss(self, y, y_cf):\n",
        "    y_cf_sig = nn.Sigmoid()(y_cf)\n",
        "    y_p_sig = nn.Sigmoid()(y)\n",
        "    fair_L = torch.sum(torch.norm(y_cf_sig - y_p_sig, p=2, dim=1))/y_cf_sig.size(0)\n",
        "    return fair_L\n",
        "\n",
        "  def calculate_loss(self, x_ind, x_desc, x_corr, x_sens, y):\n",
        "\n",
        "    # Encode\n",
        "    mu_corr, logvar_corr, mu_desc, logvar_desc = self.encode(\n",
        "        x_ind, x_desc, x_corr, x_sens, y)\n",
        "\n",
        "    # Reparamaterise\n",
        "    u_corr = self.reparameterize(mu_corr, logvar_corr)\n",
        "    u_desc = self.reparameterize(mu_desc, logvar_desc)\n",
        "\n",
        "    # Decode\n",
        "    x_corr_pred, x_desc_pred, y_pred, x_desc_cf, y_cf = self.decode(\n",
        "        u_desc, u_corr, x_ind, x_sens)\n",
        "\n",
        "    # Reconstruction & prediction loss\n",
        "    desc_recon_L = self.reconstruction_loss(x_desc_pred, x_desc, self.desc_meta)\n",
        "    corr_recon_L = self.reconstruction_loss(x_corr_pred, x_corr, self.corr_meta)\n",
        "    y_recon_L = nn.BCEWithLogitsLoss()(y_pred, y)\n",
        "\n",
        "    recon_L = self.args.desc_a*desc_recon_L + self.args.corr_a*corr_recon_L + self.args.pred_a*y_recon_L\n",
        "\n",
        "    # KL loss\n",
        "    kl_L = self.kl_loss(mu_corr, logvar_corr) + self.kl_loss(mu_desc, logvar_desc)\n",
        "\n",
        "    # TC loss\n",
        "    tc_L, disc_L = self.tc_loss(u_desc, u_corr, x_sens)\n",
        "\n",
        "    # Counterfactual fairness loss\n",
        "    fair_L = self.fair_loss(y, y_cf)\n",
        "\n",
        "    # Total VAE obective\n",
        "    # Fair Disentangled Negative ELBO = -M_ELBO + beta_tc * L_TC + beta_f * L_f\n",
        "    total_vae_loss = recon_L + kl_L + self.args.tc_b*tc_L + self.args.fair_b*fair_L\n",
        "\n",
        "    return total_vae_loss, disc_L\n",
        "\n"
      ],
      "metadata": {
        "id": "iNE0jzfR1ygY"
      },
      "id": "iNE0jzfR1ygY",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "LFHIW0ml12n3"
      },
      "id": "LFHIW0ml12n3"
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease = pd.read_csv(PROJECT_ROOT + '/data/heart_disease_cleaned.csv',\n",
        "                            dtype={'cp':int, 'ang':int, 'ecg':int, 'fbs':int, 'slope':int})\n",
        "\n",
        "feature_mapping = {\n",
        "    'ind': [{'name':'age','type':'continuous'}], # Features independent of the protected attribute and unconfounded\n",
        "    'desc': [\n",
        "        {'name':'ang','type':'binary'},\n",
        "        {'name':'cp','type':'categorical', 'n':4},\n",
        "        {'name':'ecg','type':'categorical','n':3}], # Features descendant of the protected attribute\n",
        "    'corr': [\n",
        "        {'name':'bp','type':'continuous'},\n",
        "        {'name':'chol','type':'continuous'},\n",
        "        {'name':'mhr','type':'continuous'},\n",
        "        {'name':'st','type':'continuous'},\n",
        "        {'name':'fbs','type':'binary'},\n",
        "        {'name':'slope','type':'categorical', 'n':3}], # Features correlated with the protected attribute\n",
        "    'sens': [{'name':'sex','type':'binary'}], # Sensitive attribute\n",
        "    'target': {'name':'cvd','type':'binary'} # Target outcome\n",
        "}\n",
        "\n",
        "# Bucketed data loaders for training , validation, and test\n",
        "train_loader, val_loader, test_loader = make_bucketed_loader(heart_disease, feature_mapping)\n"
      ],
      "metadata": {
        "id": "et_WZlZs12Hc"
      },
      "id": "et_WZlZs12Hc",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate the DCEVAE model\n",
        "model = DCEVAE(feature_mapping['ind'], feature_mapping['desc'], feature_mapping['corr'],\n",
        "               feature_mapping['sens'], args=args)\n",
        "\n",
        "# Training\n",
        "model.to(args.device)\n",
        "model = model.train()\n",
        "\n",
        "discrim_params = [param for name, param in model.named_parameters() if 'discriminator' in name]\n",
        "main_params = [param for name, param in model.named_parameters() if 'discriminator' not in name]\n",
        "discrim_optimiser = optim.Adam(discrim_params, lr=args.lr)\n",
        "main_optimiser = optim.Adam(main_params, lr=args.lr)\n",
        "\n",
        "for epoch in range(args.n_epochs):\n",
        "  model.train()\n",
        "  for i, (x_ind, x_desc, x_corr, x_sens, y,\n",
        "          x_ind_2, x_desc_2, x_corr_2, x_sens_2, y_2) in enumerate(train_loader):\n",
        "    elbo, disc_L = model.calculate_loss(x_ind, x_desc, x_corr, x_sens, y)\n",
        "    print(elbo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hOuetAiztsB",
        "outputId": "3c8f67b6-9318-441b-bf14-47febeff1906"
      },
      "id": "-hOuetAiztsB",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(53.9863, grad_fn=<AddBackward0>)\n",
            "tensor(53.2402, grad_fn=<AddBackward0>)\n",
            "tensor(31.4629, grad_fn=<AddBackward0>)\n",
            "tensor(47.4246, grad_fn=<AddBackward0>)\n",
            "tensor(48.1701, grad_fn=<AddBackward0>)\n",
            "tensor(35.3607, grad_fn=<AddBackward0>)\n",
            "tensor(41.1494, grad_fn=<AddBackward0>)\n",
            "tensor(34.4012, grad_fn=<AddBackward0>)\n",
            "tensor(45.0565, grad_fn=<AddBackward0>)\n",
            "tensor(44.7785, grad_fn=<AddBackward0>)\n",
            "tensor(38.4804, grad_fn=<AddBackward0>)\n",
            "tensor(40.0186, grad_fn=<AddBackward0>)\n",
            "tensor(55.5224, grad_fn=<AddBackward0>)\n",
            "tensor(61.2504, grad_fn=<AddBackward0>)\n",
            "tensor(43.2704, grad_fn=<AddBackward0>)\n",
            "tensor(99.3074, grad_fn=<AddBackward0>)\n",
            "tensor(47.6145, grad_fn=<AddBackward0>)\n",
            "tensor(72.6533, grad_fn=<AddBackward0>)\n",
            "tensor(29.2676, grad_fn=<AddBackward0>)\n",
            "tensor(41.9465, grad_fn=<AddBackward0>)\n",
            "tensor(164.3048, grad_fn=<AddBackward0>)\n",
            "tensor(46.1329, grad_fn=<AddBackward0>)\n",
            "tensor(36.6775, grad_fn=<AddBackward0>)\n",
            "tensor(39.1245, grad_fn=<AddBackward0>)\n",
            "tensor(41.4721, grad_fn=<AddBackward0>)\n",
            "tensor(40.5180, grad_fn=<AddBackward0>)\n",
            "tensor(38.4358, grad_fn=<AddBackward0>)\n",
            "tensor(45.1786, grad_fn=<AddBackward0>)\n",
            "tensor(52.3313, grad_fn=<AddBackward0>)\n",
            "tensor(56.6356, grad_fn=<AddBackward0>)\n",
            "tensor(34.0775, grad_fn=<AddBackward0>)\n",
            "tensor(30.1391, grad_fn=<AddBackward0>)\n",
            "tensor(38.7921, grad_fn=<AddBackward0>)\n",
            "tensor(47.1720, grad_fn=<AddBackward0>)\n",
            "tensor(53.7627, grad_fn=<AddBackward0>)\n",
            "tensor(62.9716, grad_fn=<AddBackward0>)\n",
            "tensor(43.6462, grad_fn=<AddBackward0>)\n",
            "tensor(56.8711, grad_fn=<AddBackward0>)\n",
            "tensor(39.6722, grad_fn=<AddBackward0>)\n",
            "tensor(34.4781, grad_fn=<AddBackward0>)\n",
            "tensor(35.0367, grad_fn=<AddBackward0>)\n",
            "tensor(41.8809, grad_fn=<AddBackward0>)\n",
            "tensor(57.1850, grad_fn=<AddBackward0>)\n",
            "tensor(41.8650, grad_fn=<AddBackward0>)\n",
            "tensor(96.3106, grad_fn=<AddBackward0>)\n",
            "tensor(48.5093, grad_fn=<AddBackward0>)\n",
            "tensor(42.3196, grad_fn=<AddBackward0>)\n",
            "tensor(57.1636, grad_fn=<AddBackward0>)\n",
            "tensor(49.4483, grad_fn=<AddBackward0>)\n",
            "tensor(40.5666, grad_fn=<AddBackward0>)\n",
            "tensor(46.0088, grad_fn=<AddBackward0>)\n",
            "tensor(66.5190, grad_fn=<AddBackward0>)\n",
            "tensor(30.5019, grad_fn=<AddBackward0>)\n",
            "tensor(36.0497, grad_fn=<AddBackward0>)\n",
            "tensor(46.2335, grad_fn=<AddBackward0>)\n",
            "tensor(53.0539, grad_fn=<AddBackward0>)\n",
            "tensor(41.6176, grad_fn=<AddBackward0>)\n",
            "tensor(36.9028, grad_fn=<AddBackward0>)\n",
            "tensor(62.5160, grad_fn=<AddBackward0>)\n",
            "tensor(41.5158, grad_fn=<AddBackward0>)\n",
            "tensor(54.2670, grad_fn=<AddBackward0>)\n",
            "tensor(66.0220, grad_fn=<AddBackward0>)\n",
            "tensor(36.1783, grad_fn=<AddBackward0>)\n",
            "tensor(170.6458, grad_fn=<AddBackward0>)\n",
            "tensor(61.1749, grad_fn=<AddBackward0>)\n",
            "tensor(38.0098, grad_fn=<AddBackward0>)\n",
            "tensor(36.9624, grad_fn=<AddBackward0>)\n",
            "tensor(31.4576, grad_fn=<AddBackward0>)\n",
            "tensor(41.4996, grad_fn=<AddBackward0>)\n",
            "tensor(53.3509, grad_fn=<AddBackward0>)\n",
            "tensor(58.6469, grad_fn=<AddBackward0>)\n",
            "tensor(58.3134, grad_fn=<AddBackward0>)\n",
            "tensor(41.5582, grad_fn=<AddBackward0>)\n",
            "tensor(38.2767, grad_fn=<AddBackward0>)\n",
            "tensor(47.1240, grad_fn=<AddBackward0>)\n",
            "tensor(29.1679, grad_fn=<AddBackward0>)\n",
            "tensor(38.7347, grad_fn=<AddBackward0>)\n",
            "tensor(52.2098, grad_fn=<AddBackward0>)\n",
            "tensor(46.1143, grad_fn=<AddBackward0>)\n",
            "tensor(50.0181, grad_fn=<AddBackward0>)\n",
            "tensor(53.9481, grad_fn=<AddBackward0>)\n",
            "tensor(68.6000, grad_fn=<AddBackward0>)\n",
            "tensor(35.3001, grad_fn=<AddBackward0>)\n",
            "tensor(61.6728, grad_fn=<AddBackward0>)\n",
            "tensor(39.8852, grad_fn=<AddBackward0>)\n",
            "tensor(38.1333, grad_fn=<AddBackward0>)\n",
            "tensor(45.6322, grad_fn=<AddBackward0>)\n",
            "tensor(39.7701, grad_fn=<AddBackward0>)\n",
            "tensor(40.3010, grad_fn=<AddBackward0>)\n",
            "tensor(46.9164, grad_fn=<AddBackward0>)\n",
            "tensor(37.2194, grad_fn=<AddBackward0>)\n",
            "tensor(38.4336, grad_fn=<AddBackward0>)\n",
            "tensor(39.7751, grad_fn=<AddBackward0>)\n",
            "tensor(33.9465, grad_fn=<AddBackward0>)\n",
            "tensor(54.3345, grad_fn=<AddBackward0>)\n",
            "tensor(58.4927, grad_fn=<AddBackward0>)\n",
            "tensor(39.4109, grad_fn=<AddBackward0>)\n",
            "tensor(42.4359, grad_fn=<AddBackward0>)\n",
            "tensor(38.7734, grad_fn=<AddBackward0>)\n",
            "tensor(29.1565, grad_fn=<AddBackward0>)\n",
            "tensor(47.3621, grad_fn=<AddBackward0>)\n",
            "tensor(77.1515, grad_fn=<AddBackward0>)\n",
            "tensor(62.8394, grad_fn=<AddBackward0>)\n",
            "tensor(37.4707, grad_fn=<AddBackward0>)\n",
            "tensor(51.9941, grad_fn=<AddBackward0>)\n",
            "tensor(51.0194, grad_fn=<AddBackward0>)\n",
            "tensor(39.5242, grad_fn=<AddBackward0>)\n",
            "tensor(86.7671, grad_fn=<AddBackward0>)\n",
            "tensor(43.8931, grad_fn=<AddBackward0>)\n",
            "tensor(44.9844, grad_fn=<AddBackward0>)\n",
            "tensor(64.5090, grad_fn=<AddBackward0>)\n",
            "tensor(34.5392, grad_fn=<AddBackward0>)\n",
            "tensor(36.8695, grad_fn=<AddBackward0>)\n",
            "tensor(37.1829, grad_fn=<AddBackward0>)\n",
            "tensor(42.0388, grad_fn=<AddBackward0>)\n",
            "tensor(47.2490, grad_fn=<AddBackward0>)\n",
            "tensor(33.1345, grad_fn=<AddBackward0>)\n",
            "tensor(35.9965, grad_fn=<AddBackward0>)\n",
            "tensor(55.9905, grad_fn=<AddBackward0>)\n",
            "tensor(43.1798, grad_fn=<AddBackward0>)\n",
            "tensor(47.0336, grad_fn=<AddBackward0>)\n",
            "tensor(53.2587, grad_fn=<AddBackward0>)\n",
            "tensor(39.0484, grad_fn=<AddBackward0>)\n",
            "tensor(48.0002, grad_fn=<AddBackward0>)\n",
            "tensor(40.6478, grad_fn=<AddBackward0>)\n",
            "tensor(39.9276, grad_fn=<AddBackward0>)\n",
            "tensor(43.7880, grad_fn=<AddBackward0>)\n",
            "tensor(106.7489, grad_fn=<AddBackward0>)\n",
            "tensor(47.6970, grad_fn=<AddBackward0>)\n",
            "tensor(46.8020, grad_fn=<AddBackward0>)\n",
            "tensor(41.4009, grad_fn=<AddBackward0>)\n",
            "tensor(38.2444, grad_fn=<AddBackward0>)\n",
            "tensor(36.3379, grad_fn=<AddBackward0>)\n",
            "tensor(38.0533, grad_fn=<AddBackward0>)\n",
            "tensor(40.0058, grad_fn=<AddBackward0>)\n",
            "tensor(36.9319, grad_fn=<AddBackward0>)\n",
            "tensor(63.7842, grad_fn=<AddBackward0>)\n",
            "tensor(50.2867, grad_fn=<AddBackward0>)\n",
            "tensor(42.1046, grad_fn=<AddBackward0>)\n",
            "tensor(54.0344, grad_fn=<AddBackward0>)\n",
            "tensor(42.6077, grad_fn=<AddBackward0>)\n",
            "tensor(57.5471, grad_fn=<AddBackward0>)\n",
            "tensor(39.7368, grad_fn=<AddBackward0>)\n",
            "tensor(50.9829, grad_fn=<AddBackward0>)\n",
            "tensor(54.3512, grad_fn=<AddBackward0>)\n",
            "tensor(38.8605, grad_fn=<AddBackward0>)\n",
            "tensor(32.4817, grad_fn=<AddBackward0>)\n",
            "tensor(36.2709, grad_fn=<AddBackward0>)\n",
            "tensor(42.0248, grad_fn=<AddBackward0>)\n",
            "tensor(36.0367, grad_fn=<AddBackward0>)\n",
            "tensor(38.6869, grad_fn=<AddBackward0>)\n",
            "tensor(43.2086, grad_fn=<AddBackward0>)\n",
            "tensor(36.5706, grad_fn=<AddBackward0>)\n",
            "tensor(47.2087, grad_fn=<AddBackward0>)\n",
            "tensor(58.5781, grad_fn=<AddBackward0>)\n",
            "tensor(41.6260, grad_fn=<AddBackward0>)\n",
            "tensor(42.6252, grad_fn=<AddBackward0>)\n",
            "tensor(37.0979, grad_fn=<AddBackward0>)\n",
            "tensor(51.1911, grad_fn=<AddBackward0>)\n",
            "tensor(40.3431, grad_fn=<AddBackward0>)\n",
            "tensor(47.1915, grad_fn=<AddBackward0>)\n",
            "tensor(173.4563, grad_fn=<AddBackward0>)\n",
            "tensor(45.2277, grad_fn=<AddBackward0>)\n",
            "tensor(43.5694, grad_fn=<AddBackward0>)\n",
            "tensor(42.6606, grad_fn=<AddBackward0>)\n",
            "tensor(38.5571, grad_fn=<AddBackward0>)\n",
            "tensor(39.2555, grad_fn=<AddBackward0>)\n",
            "tensor(46.2307, grad_fn=<AddBackward0>)\n",
            "tensor(39.7588, grad_fn=<AddBackward0>)\n",
            "tensor(39.2600, grad_fn=<AddBackward0>)\n",
            "tensor(31.7683, grad_fn=<AddBackward0>)\n",
            "tensor(104.5017, grad_fn=<AddBackward0>)\n",
            "tensor(49.5912, grad_fn=<AddBackward0>)\n",
            "tensor(33.3485, grad_fn=<AddBackward0>)\n",
            "tensor(38.7787, grad_fn=<AddBackward0>)\n",
            "tensor(86.2162, grad_fn=<AddBackward0>)\n",
            "tensor(47.6361, grad_fn=<AddBackward0>)\n",
            "tensor(59.1134, grad_fn=<AddBackward0>)\n",
            "tensor(52.1468, grad_fn=<AddBackward0>)\n",
            "tensor(37.0949, grad_fn=<AddBackward0>)\n",
            "tensor(40.4677, grad_fn=<AddBackward0>)\n",
            "tensor(42.9829, grad_fn=<AddBackward0>)\n",
            "tensor(52.2863, grad_fn=<AddBackward0>)\n",
            "tensor(39.6595, grad_fn=<AddBackward0>)\n",
            "tensor(63.8425, grad_fn=<AddBackward0>)\n",
            "tensor(47.9011, grad_fn=<AddBackward0>)\n",
            "tensor(37.5045, grad_fn=<AddBackward0>)\n",
            "tensor(35.5343, grad_fn=<AddBackward0>)\n",
            "tensor(36.9266, grad_fn=<AddBackward0>)\n",
            "tensor(44.0583, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}