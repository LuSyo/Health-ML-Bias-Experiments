{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec5acca",
   "metadata": {},
   "source": [
    "# Training a counterfactually fair CAD model\n",
    "\n",
    "## Counterfactual Fairness\n",
    "\n",
    "Attempts to mitigate bias in a model by simply removing sensitive attributes from its training, i.e. fairness by unawareness, often fails due to bias 'leaking' through causal relationships between the sensitive attribute and other features retained in the data. The counterfactual fairness approach introduced by Kusner et al. (2017, in Advances in Neural Information Processing Systems, https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) addresses this limitation by deconvoluting the biased observed variables into a fair set of unbiased latent variables. It allows the model to only learn from information that is independent from the protected attribute, and neutralise both direct and proxy bias pathways.\n",
    "\n",
    "### Notations and definitions\n",
    "\n",
    "we adopt the following notations consistent with the Pearlian causal framework used by Kusner et al.:\n",
    "\n",
    "- $A$ **Protected attribute**: The sensitive variable we wish to be fair toward\n",
    "- $X$ **Observed features**: The set of features available in the dataset (e.g. Blood Pressure, Cholesterol)\n",
    "- $U$ **Latent (unobserved) variables**: Unobserved variables that are independent of the protected attribute $A$\n",
    "- $Y$ **Target**: The outcome we are predicting (e.g. Presence of CAD)\n",
    "- $Y_{A \\leftarrow a}$: The value of $Y$ under a counterfactual intervention where $A$ is set to $a$\n",
    "\n",
    "**Counterfactual Fairness:** A predictor $\\hat{Y}$ is counterfactually fair if, for a specific individual, the probability distribution of the prediction is the same in the actual world as it would be in a counterfactual world where their protected attribute (e.g., Sex) was different.\n",
    "\n",
    "Formally, for any value $a'$ of the protected attribute $A$:$$P(\\hat{Y}_{A \\leftarrow a} = y \\mid X=x, A=a) = P(\\hat{Y}_{A \\leftarrow a'} = y \\mid X=x, A=a)$$\n",
    "\n",
    "\n",
    "## The experiment\n",
    "\n",
    "Using the fairness-unaware models trained to predict Cardiovascular Disease in Straw et al. (2024, doi: [10.2196/46936](https://doi.org/10.2196/46936)) as baseline models, we will apply the fairness algorithm proposed by Kusner et al. to train a fair CAD predictor. \n",
    "\n",
    "### The target bias\n",
    "While Kusner focus on mitigating bias on tasks where the protected attribute should have no influence on the target outcome (e.g. sex and exam results), the clinical domain brings a new challenge. Indeed, protected attributes such as sex or race oftne encompass two variables: the clinically relevant biological attribute which can cause a disease to present differently across individuals, and the sociological attribute which has societal factors that can influence healthcare access, physician perception, diagnosis and care. A clinical outcome might be influenced by the former but should remain independent of the latter. \n",
    "\n",
    "If we aim for fairness based on the high-level sex attribute, we risk removing legitimate clinical signals and degrading diagnostic accuracy. Therefore, our objective in experiment is to make the model counterfactually fair with regards to **sociological sex**. \n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "By using counterfactual inference to model latent variables that are independent of the protected attribute, we can build a predictor that satisfies counterfactual fairness (i.e. ensuring that an individualâ€™s predicted risk of CAD remains invariant to their sociological sex), while maintaining clinically acceptable predictive performance and reducing the False Negative Rate (FNR) disparity observed in baseline models.\n",
    "\n",
    "## Causal model\n",
    "\n",
    "From the feature set observed in the [Heart Disease (CAD) dataset](https://ieee-dataport.org/open-access/heart-disease-dataset-comprehensive), using clinical knowledge in the literature and strong assumptions, we create two causal models that we will compare in this experiment:\n",
    "\n",
    "### Latent variables model\n",
    "\n",
    "\n",
    "### Additive error model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
